{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDT4173 - Machine Learning\n",
    "Names: Jacob Alexander Worsøe, Henrik August Søntvedt, Simen Øygarden Burgos\n",
    "\n",
    "Student IDs: 544553, 543969, 543917\n",
    "\n",
    "Team name: 5tabekk\n",
    "\n",
    "Team number: 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Long Notebook \n",
    "\n",
    "We worked with a system where we created a python library containing helper functions. These have been placed below and adapted to the notebook format. This library also contained a MetaModel class that we created to more easily test and keep track of our models. This is why our models are written into classes.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Data location\n",
    "\n",
    "We kept the data in a folder titled data as follows\n",
    " - data\n",
    "    - A\n",
    "    - B\n",
    "    - C\n",
    "    my_first_submission.csv\n",
    "    read_files.ipynb\n",
    "    Readme.md\n",
    "    sample_submission.csv\n",
    "    test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import statistics\n",
    "\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "from keras import models, layers, optimizers, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "import lightgbm as lgbm\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.utils import timeseries_dataset_from_array\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "from prophet import Prophet\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from itertools import combinations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data fetching and preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_training():\n",
    "    \"\"\" gets full training data (duh) \"\"\"\n",
    "\n",
    "    train_a = pd.read_parquet(data_dir + 'A/train_targets.parquet')\n",
    "    train_b = pd.read_parquet(data_dir + 'B/train_targets.parquet')\n",
    "    train_c = pd.read_parquet(data_dir + 'C/train_targets.parquet')\n",
    "\n",
    "    X_train_estimated_a = pd.read_parquet(data_dir + 'A/X_train_estimated.parquet')\n",
    "    X_train_estimated_b = pd.read_parquet(data_dir + 'B/X_train_estimated.parquet')\n",
    "    X_train_estimated_c = pd.read_parquet(data_dir + 'C/X_train_estimated.parquet')\n",
    "\n",
    "    X_train_observed_a = pd.read_parquet(data_dir + 'A/X_train_observed.parquet')\n",
    "    X_train_observed_b = pd.read_parquet(data_dir + 'B/X_train_observed.parquet')\n",
    "    X_train_observed_c = pd.read_parquet(data_dir + 'C/X_train_observed.parquet')\n",
    "\n",
    "    ret = pd.DataFrame()\n",
    "    ret.set_index(pd.MultiIndex(levels=[[],[]], codes=[[],[]], names=['location', 'datetime']), inplace=True)\n",
    "    ret.columns = pd.MultiIndex(levels=[[],[], []], codes=[[],[], []], names=['feature_type', 'minutes', 'feature_name'])\n",
    "\n",
    "\n",
    "    # estimated data\n",
    "    data_dict = {'A': X_train_estimated_a, 'B': X_train_estimated_b, 'C': X_train_estimated_c}\n",
    "\n",
    "    for loc in data_dict:\n",
    "        out_temp = pd.DataFrame(data_dict[loc].date_forecast.dt.floor('H').unique(), columns=['date_forecast'])\n",
    "        out_temp.set_index('date_forecast', inplace=True)\n",
    "        out_temp.columns = pd.MultiIndex.from_product([[], [], out_temp.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "\n",
    "        for m in [0, 15, 30, 45]:\n",
    "            temp = data_dict[loc][data_dict[loc].date_forecast.dt.minute == m].copy()\n",
    "            temp.set_index(temp.date_forecast.dt.floor('H'), inplace=True)\n",
    "            temp.drop(columns=['date_calc', 'date_forecast'], inplace=True)\n",
    "            temp.columns = pd.MultiIndex.from_product([['estimated'], [m], temp.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "            out_temp = out_temp.merge(temp,left_index=True, right_index=True, how='outer')\n",
    "\n",
    "        out_temp.set_index(pd.MultiIndex.from_product([[loc], out_temp.index], names=['location', 'datetime']), inplace=True)\n",
    "        ret = pd.concat([ret, out_temp])\n",
    "\n",
    "    # observed data\n",
    "    data_dict = {'A': X_train_observed_a, 'B': X_train_observed_b, 'C': X_train_observed_c}\n",
    "\n",
    "    for loc in data_dict:\n",
    "        out_temp = pd.DataFrame(data_dict[loc].date_forecast.dt.floor('H').unique(), columns=['date_forecast'])\n",
    "        out_temp.set_index('date_forecast', inplace=True)\n",
    "        out_temp.columns = pd.MultiIndex.from_product([[], [], out_temp.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "\n",
    "        for m in [0, 15, 30, 45]:\n",
    "            temp = data_dict[loc][data_dict[loc].date_forecast.dt.minute == m].copy()\n",
    "            temp.set_index(temp.date_forecast.dt.floor('H'), inplace=True)\n",
    "            temp.drop(columns=['date_forecast'], inplace=True)\n",
    "            temp.columns = pd.MultiIndex.from_product([['observed'], [m], temp.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "            out_temp = out_temp.merge(temp,left_index=True, right_index=True, how='outer')\n",
    "\n",
    "        out_temp.set_index(pd.MultiIndex.from_product([[loc], out_temp.index], names=['location', 'datetime']), inplace=True)\n",
    "        ret = pd.concat([ret, out_temp])\n",
    "        \n",
    "\n",
    "\n",
    "    # # train data\n",
    "    data_dict = {'B': train_b, 'C': train_c} # 'A': train_a, \n",
    "\n",
    "    out_temp = train_a.dropna().copy()\n",
    "    out_temp.rename(columns={'time': 'datetime'}, inplace=True)\n",
    "    out_temp.set_index('datetime', inplace=True)\n",
    "    out_temp.columns = pd.MultiIndex.from_product([['y'], ['NA'], out_temp.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "\n",
    "    out_temp.set_index(pd.MultiIndex.from_product([['A'], out_temp.index], names=['location', 'datetime']), inplace=True)\n",
    "\n",
    "    for loc in data_dict:\n",
    "\n",
    "        out_temp2 = data_dict[loc].dropna().copy()\n",
    "        out_temp2.rename(columns={'time': 'datetime'}, inplace=True)\n",
    "        out_temp2.set_index('datetime', inplace=True)\n",
    "        out_temp2.columns = pd.MultiIndex.from_product([['y'], ['NA'], out_temp2.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "\n",
    "        out_temp2.set_index(pd.MultiIndex.from_product([[loc], out_temp2.index], names=['location', 'datetime']), inplace=True)\n",
    "\n",
    "        out_temp = pd.concat([out_temp, out_temp2])\n",
    "\n",
    "        # ret = ret.merge(out_temp, left_index=True, right_index=True, how='outer')\n",
    "        # ret = pd.concat([ret, out_temp])\n",
    "        \n",
    "\n",
    "    ret = pd.merge(out_temp, ret, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_training_groupby_mean():\n",
    "    return get_training().groupby(level = [0,2], axis = 1).mean()[['y', 'estimated', 'observed']]\n",
    "\n",
    "\n",
    "def get_training_flattened():\n",
    "    df = get_training_groupby_mean()\n",
    "\n",
    "    df['estimated', 'weather_data_type'] = np.nan\n",
    "\n",
    "    df['estimated', 'weather_data_type'] = np.where(df['observed', 'absolute_humidity_2m:gm3'].notna(),'observed', df['estimated', 'weather_data_type'])\n",
    "    df['estimated', 'weather_data_type'] = np.where(df['estimated', 'absolute_humidity_2m:gm3'].notna(),'estimated', df['estimated', 'weather_data_type'])\n",
    "    df.estimated = df.estimated.fillna(df.observed)\n",
    "\n",
    "    df = df.drop(columns=['observed'], level=0)\n",
    "    df.columns = df.columns.droplevel().tolist()\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'datetime': 'ds', 'pv_measurement': 'y'}, inplace=True)\n",
    "    # df['ENG_total_rad'] = df['diffuse_rad_1h:J'] + df['direct_rad_1h:J']\n",
    "    \n",
    "    return df[['location', 'ds', 'y', 'weather_data_type'] + [i for i in df.columns.tolist() if i not in ['location', 'ds', 'y', 'weather_data_type']]].copy()\n",
    "\n",
    "def get_training_cleaned():\n",
    "    df = get_training_flattened()\n",
    "\n",
    "    df = df[\n",
    "            ~((df.y != 0) & \n",
    "            (df.y == df.y.shift(-1)) &\n",
    "            (df.y == df.y.shift(-2)))\n",
    "        ].copy()\n",
    "\n",
    "    df.dropna(axis=0, subset='y', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "### Testing\n",
    "\n",
    "def get_testing():\n",
    "    \"\"\" gets the feature estimates used for the forecast \"\"\"\n",
    "\n",
    "    X_test_estimated_a = pd.read_parquet(data_dir + 'A/X_test_estimated.parquet')\n",
    "    X_test_estimated_b = pd.read_parquet(data_dir + 'B/X_test_estimated.parquet')\n",
    "    X_test_estimated_c = pd.read_parquet(data_dir + 'C/X_test_estimated.parquet')\n",
    "\n",
    "    ret = pd.DataFrame()\n",
    "    ret.set_index(pd.MultiIndex(levels=[[],[]], codes=[[],[]], names=['location', 'datetime']), inplace=True)\n",
    "    ret.columns = pd.MultiIndex(levels=[[],[], []], codes=[[],[], []], names=['feature_type', 'minutes', 'feature_name'])\n",
    "\n",
    "\n",
    "    # estimated data\n",
    "    data_dict = {'A': X_test_estimated_a, 'B': X_test_estimated_b, 'C': X_test_estimated_c}\n",
    "\n",
    "    for loc in data_dict:\n",
    "        out_temp = pd.DataFrame(data_dict[loc].date_forecast.dt.floor('H').unique(), columns=['date_forecast'])\n",
    "        out_temp.set_index('date_forecast', inplace=True)\n",
    "        out_temp.columns = pd.MultiIndex.from_product([[], [], out_temp.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "\n",
    "        for m in [0, 15, 30, 45]:\n",
    "            temp = data_dict[loc][data_dict[loc].date_forecast.dt.minute == m].copy()\n",
    "            temp.set_index(temp.date_forecast.dt.floor('H'), inplace=True)\n",
    "            temp.drop(columns=['date_calc', 'date_forecast'], inplace=True)\n",
    "            temp.columns = pd.MultiIndex.from_product([['estimated'], [m], temp.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "            out_temp = out_temp.merge(temp,left_index=True, right_index=True, how='outer')\n",
    "\n",
    "        out_temp.set_index(pd.MultiIndex.from_product([[loc], out_temp.index], names=['location', 'datetime']), inplace=True)\n",
    "        ret = pd.concat([ret, out_temp])\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_testing_flattened():\n",
    "    df = get_testing()\n",
    "    df = df.groupby(level = [0,2], axis = 1).mean()[['estimated']]\n",
    "    df.columns = df.columns.droplevel().tolist()\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'datetime': 'ds'}, inplace=True)\n",
    "    df['weather_data_type'] = 'estimated'\n",
    "    return df[['location', 'ds', 'weather_data_type'] + [i for i in df.columns.tolist() if i not in ['location', 'ds', 'y', 'weather_data_type']]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other helper functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Avoiding Deductions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Exploratory Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.1 Search domain knowledge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.2 Check if the data is intuitive\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.3 Understand how the data was generated \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.4 Explore individual features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.5 Explore pairs and groups of features \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.6 Clean up features \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Predictors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two examples of predictors used: \n",
    "\n",
    "1. Linear regression - see model 2\n",
    "\n",
    "    Simple linear regression model using total radiation (direct + diffuse) as the independent variable and pv_measurement as the target variable.\n",
    "\n",
    "2. XGBoost (eXtreme Gradient Boosting) - see model 4 and 5\n",
    "\n",
    "     Uses gradient boosting to build an ensemble of decision trees, each trained on a subset of the data. The final prediction is a combination of the individual predictions from distinct trees."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on initial EDA, several of the original features were excluded:\n",
    "\n",
    "- of the five fresh_snow columns, all except fresh_snow_3h:cm were discarded. This decision was based on the fact that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Initial colums\n",
    "\"\"\"\n",
    "Index(['location', 'ds', 'y', 'weather_data_type', 'absolute_humidity_2m:gm3',\n",
    "       'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J',\n",
    "       'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx',\n",
    "       'dew_point_2m:K', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W',\n",
    "       'direct_rad_1h:J', 'effective_cloud_cover:p', 'elevation:m',\n",
    "       'fresh_snow_12h:cm', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm',\n",
    "       'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'is_day:idx',\n",
    "       'is_in_shadow:idx', 'msl_pressure:hPa', 'precip_5min:mm',\n",
    "       'precip_type_5min:idx', 'pressure_100m:hPa', 'pressure_50m:hPa',\n",
    "       'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "       'sfc_pressure:hPa', 'snow_density:kgm3', 'snow_depth:cm',\n",
    "       'snow_drift:idx', 'snow_melt_10min:mm', 'snow_water:kgm2',\n",
    "       'sun_azimuth:d', 'sun_elevation:d', 'super_cooled_liquid_water:kgm2',\n",
    "       't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "       'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms',\n",
    "       'wind_speed_w_1000hPa:ms', 'total_rad_1h:J'],\n",
    "      dtype='object')\n",
    "\"\"\"\n",
    "### Columns in cat_composite after feature selection\n",
    "\"\"\"\n",
    "self.common_features = ['sample_importance', 'is_estimated','dayofyear',\n",
    "                                'is_day:idx',\n",
    "                             'hour', 'month',\n",
    "                            'total_rad_1h:J',\n",
    "                            'sun_elevation:d',\n",
    "                            'sun_azimuth:d',\n",
    "                            'is_in_shadow:idx',\n",
    "                            'effective_cloud_cover:p']\n",
    "        \n",
    "self.random_features = ['absolute_humidity_2m:gm3',\n",
    "                                'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J',\n",
    "                                'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx',\n",
    "                                'dew_point_2m:K', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W',\n",
    "                                'direct_rad_1h:J', 'fresh_snow_3h:cm',\n",
    "                                'precip_5min:mm','precip_type_5min:idx', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "                                'sfc_pressure:hPa','snow_water:kgm2',\n",
    "                                'super_cooled_liquid_water:kgm2',\n",
    "                                't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "                                'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms']\n",
    "\"\"\"                               \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Model Interpretation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Models\n",
    "This section contains all model classes used to generate submitted Kaggle predictions. Models are ordered chronologically by the first date they were used to generate a submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MetaModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: \n",
    "Abstract class which provides a framework for implementing subsequent models. MetaModel contains a test function common to all classes, which standardizes how models are evaluated (MAE-based). It also contains three abstract functions which are implemented in the actual models, namely preprocess, train, and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaModel(ABC):     \n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "\n",
    "        return\n",
    "\n",
    "    # Denne df-en må ha y som kolonne\n",
    "    def test(self, df: pd.DataFrame, n_splits=5):\n",
    "        \"\"\"\n",
    "            Expanding window cross-validation, df must have y in it for testing against predictions\n",
    "        \"\"\"\n",
    "        print(f\"Testing {self.model_name}\")\n",
    "        column_names = df.columns.tolist()\n",
    "        if 'y' not in column_names:\n",
    "            raise Exception(f\"Missing observed y in columns. Available are {column_names}\")\n",
    "\n",
    "        # This is unecessary because we already clean it when calling train\n",
    "        # drop_y_with_na\n",
    "        df = df.dropna(subset=['y'], inplace=False)\n",
    "\n",
    "        MAE_values = []\n",
    "        MSE_values = []\n",
    "\n",
    "        # tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        kf =KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "        for train_index, test_index in kf.split(df):\n",
    "            train_partition = df.iloc[train_index]\n",
    "            valid_partition = df.iloc[test_index]\n",
    "\n",
    "            self.train(train_partition)\n",
    "            predictions = self.predict(valid_partition)\n",
    "            \n",
    "            y_true = valid_partition['y']\n",
    "            y_pred = predictions['y_pred']\n",
    "\n",
    "            MAE = mean_absolute_error(y_true, y_pred)\n",
    "            MAE_values.append(MAE)\n",
    "\n",
    "            MSE_values.append((y_pred - y_true).mean())\n",
    "\n",
    "            print(f\"Run {len(MAE_values)} MAE =\", MAE)\n",
    "\n",
    "        print(\"Mean Signed Error vals\", MSE_values)\n",
    "        average_mae = statistics.mean(MAE_values)\n",
    "        print(\"MAE Vals: MEAN:\", average_mae, 'ALL:' , MAE_values)\n",
    "        \n",
    "        return MAE_values\n",
    "    \n",
    "    @abstractmethod\n",
    "    def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "            Takes in single-index (datetime as index) df, and returns df with only desired features\n",
    "        \"\"\"    \n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def train(df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "            Runs trained model on on input df, preprocessing the df first and then returns datetime and y_pred\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Naive estimator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: \n",
    "For each location, the model predicts pv_measurement to be the average pv_measurement for that location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveModel(MetaModel):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"Naive Model\")\n",
    "        \n",
    "    def preprocess(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        temp_df = df.copy()\n",
    "\n",
    "        return temp_df\n",
    "\n",
    "    def train(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        df = self.preprocess(df)\n",
    "\n",
    "        self.model = df.y.mean()\n",
    "\n",
    "\n",
    "    def predict(self, df):\n",
    "        df = self.preprocess(df)\n",
    "\n",
    "        df['y_pred'] = self.model\n",
    "\n",
    "        return df[['ds', 'y_pred']].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(MetaModel):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"Linear Regression\")\n",
    "        \n",
    "    def preprocess(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        temp_df = df.copy()\n",
    "\n",
    "        temp_df['total_rad_1h:J'] = df['diffuse_rad_1h:J'] + df['direct_rad_1h:J']\n",
    "\n",
    "        temp_df['total_rad_1h:J'].ffill(inplace=True)\n",
    "        temp_df['total_rad_1h:J'].bfill(inplace=True)\n",
    "\n",
    "        temp_df['total_rad_1h:J'].fillna(temp_df['total_rad_1h:J'].interpolate().cummax(), inplace=True)\n",
    "\n",
    "        if('y' in temp_df.columns.tolist()):\n",
    "            temp_df['y'].fillna(df['y'].interpolate().cummax(), inplace=True)\n",
    "\n",
    "        return temp_df\n",
    "\n",
    "    def train(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        df = self.preprocess(df)\n",
    "\n",
    "        self.model = LinearRegression()\n",
    "        self.model.fit(df['total_rad_1h:J'].values.reshape(-1, 1), df['y'].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "    def predict(self, df):\n",
    "        df = self.preprocess(df)\n",
    "\n",
    "        y_preds = self.model.predict(df['total_rad_1h:J'].values.reshape(-1, 1))\n",
    "        y_preds_arr = np.array(y_preds)\n",
    "\n",
    "        out_df = pd.DataFrame(y_preds_arr, columns=[\"y_pred\"])        \n",
    "\n",
    "        return out_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# Get the absolute path of folder2\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m current_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(\u001b[39m__file__\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m parent_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(current_dir, os\u001b[39m.\u001b[39mpardir)\n\u001b[0;32m      8\u001b[0m \u001b[39m# Add to sys.path\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "class ProphetModel(MetaModel):\n",
    "    \"\"\"\n",
    "    Hyperparameters, default in parentheses:\n",
    "\n",
    "        'C_observed_only' (False) - whether location C should only use observed data\n",
    "        'features_by_corr0.6' (True) - use features with corr > 0.6 with y, if false uses 'total_rad_1h:J'\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hyperparameters = None):\n",
    "        super().__init__(\"Prophet Model\")\n",
    "        self.hyperparameters = hyperparameters\n",
    "        \n",
    "    def preprocess(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "\n",
    "        if self.hyperparameters is not None and self.hyperparameters[\"C_observed_only\"]:\n",
    "            if location == \"C\":\n",
    "                df = df[df.weather_data_type == 'observed'].copy()\n",
    "\n",
    "\n",
    "        df['total_rad_1h:J'] = df['diffuse_rad_1h:J'] + df['direct_rad_1h:J']\n",
    "\n",
    "        df = df.dropna(axis=0, how=\"all\", subset=\"total_rad_1h:J\")\n",
    "\n",
    "        if('y' in df.columns.tolist()):\n",
    "            df = df.dropna(axis=0, how=\"all\", subset=\"y\")\n",
    "\n",
    "        df.fillna(0, inplace=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def train(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        df = self.preprocess(df)\n",
    "\n",
    "        self.prophet_model = Prophet()\n",
    "        \n",
    "        feat_find = df.corr().y.apply(abs).sort_values(ascending=False)\n",
    "        features = feat_find[feat_find > 0.6].index.tolist()\n",
    "        features.remove(\"y\")\n",
    "        \n",
    "        if self.hyperparameters is not None and not self.hyperparameters[\"features_by_corr0.6\"]:\n",
    "            features = ['total_rad_1h:J']\n",
    "        \n",
    "        for feat in features:\n",
    "            self.prophet_model.add_regressor(feat)\n",
    "\n",
    "        self.prophet_model.fit(df)\n",
    "\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        df = self.preprocess(df)\n",
    "\n",
    "        forecast = self.prophet_model.predict(df)\n",
    "        # self.prophet_model.plot_components(forecast)\n",
    "        # fig = prophet_model.plot_components(forecast)\n",
    "        temp_ret = forecast[['ds', 'yhat']].rename(columns={'yhat':'y_pred'})\n",
    "\n",
    "        # force negative values to zero\n",
    "        temp_ret.y_pred = temp_ret.y_pred.apply(lambda a : max(a, 0))\n",
    "\n",
    "        return temp_ret"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 XG Boost Rev1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Import libraries\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mxgboost\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mxgb\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Simen\\Programs\\Lib\\site-packages\\pandas\\__init__.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[0;32m     21\u001b[0m \u001b[39m# numpy compat\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m is_numpy_dev \u001b[39mas\u001b[39;00m _is_numpy_dev  \u001b[39m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_libs\u001b[39;00m \u001b[39mimport\u001b[39;00m hashtable \u001b[39mas\u001b[39;00m _hashtable, lib \u001b[39mas\u001b[39;00m _lib, tslib \u001b[39mas\u001b[39;00m _tslib\n",
      "File \u001b[1;32mc:\\Users\\Simen\\Programs\\Lib\\site-packages\\pandas\\compat\\__init__.py:25\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_constants\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     IS64,\n\u001b[0;32m     19\u001b[0m     PY39,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     PYPY,\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompressors\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnumpy\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     26\u001b[0m     is_numpy_dev,\n\u001b[0;32m     27\u001b[0m     np_version_under1p21,\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyarrow\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     30\u001b[0m     pa_version_under7p0,\n\u001b[0;32m     31\u001b[0m     pa_version_under8p0,\n\u001b[0;32m     32\u001b[0m     pa_version_under9p0,\n\u001b[0;32m     33\u001b[0m     pa_version_under11p0,\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_function_name\u001b[39m(f: F, name: \u001b[39mstr\u001b[39m, \u001b[39mcls\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m F:\n",
      "File \u001b[1;32mc:\\Users\\Simen\\Programs\\Lib\\site-packages\\pandas\\compat\\numpy\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\" support numpy compatibility across versions \"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m Version\n\u001b[0;32m      6\u001b[0m \u001b[39m# numpy versioning\u001b[39;00m\n\u001b[0;32m      7\u001b[0m _np_version \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39m__version__\n",
      "File \u001b[1;32mc:\\Users\\Simen\\Programs\\Lib\\site-packages\\pandas\\util\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# pyright: reportUnusedImport = false\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_decorators\u001b[39;00m \u001b[39mimport\u001b[39;00m (  \u001b[39m# noqa:F401\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     Appender,\n\u001b[0;32m      4\u001b[0m     Substitution,\n\u001b[0;32m      5\u001b[0m     cache_readonly,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhashing\u001b[39;00m \u001b[39mimport\u001b[39;00m (  \u001b[39m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     hash_array,\n\u001b[0;32m     10\u001b[0m     hash_pandas_object,\n\u001b[0;32m     11\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Simen\\Programs\\Lib\\site-packages\\pandas\\util\\_decorators.py:14\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     Any,\n\u001b[0;32m      8\u001b[0m     Callable,\n\u001b[0;32m      9\u001b[0m     Mapping,\n\u001b[0;32m     10\u001b[0m     cast,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_libs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproperties\u001b[39;00m \u001b[39mimport\u001b[39;00m cache_readonly\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_typing\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     F,\n\u001b[0;32m     17\u001b[0m     T,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_exceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m find_stack_level\n",
      "File \u001b[1;32mc:\\Users\\Simen\\Programs\\Lib\\site-packages\\pandas\\_libs\\__init__.py:13\u001b[0m\n\u001b[0;32m      1\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mNaT\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mNaTType\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mInterval\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m ]\n\u001b[1;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_libs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minterval\u001b[39;00m \u001b[39mimport\u001b[39;00m Interval\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_libs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtslibs\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     15\u001b[0m     NaT,\n\u001b[0;32m     16\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     iNaT,\n\u001b[0;32m     22\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Simen\\Programs\\Lib\\site-packages\\pandas\\_libs\\interval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Simen\\Programs\\Lib\\site-packages\\pandas\\_libs\\hashtable.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.hashtable\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Simen\\Programs\\Lib\\site-packages\\pandas\\_libs\\missing.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.missing\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Simen\\Programs\\Lib\\site-packages\\pandas\\_libs\\tslibs\\__init__.py:78\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_libs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtslibs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtimezones\u001b[39;00m \u001b[39mimport\u001b[39;00m tz_compare\n\u001b[0;32m     77\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_libs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtslibs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtzconversion\u001b[39;00m \u001b[39mimport\u001b[39;00m tz_convert_from_utc_single\n\u001b[1;32m---> 78\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_libs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtslibs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvectorized\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     79\u001b[0m     dt64arr_to_periodarr,\n\u001b[0;32m     80\u001b[0m     get_resolution,\n\u001b[0;32m     81\u001b[0m     ints_to_pydatetime,\n\u001b[0;32m     82\u001b[0m     is_date_array_normalized,\n\u001b[0;32m     83\u001b[0m     normalize_i8_timestamps,\n\u001b[0;32m     84\u001b[0m     tz_convert_from_utc,\n\u001b[0;32m     85\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:405\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class XGBoostModel(MetaModel):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"XGBoost\")\n",
    "\n",
    "    def preprocess(self, df):\n",
    "        temp_df = df.copy()\n",
    "\n",
    "        # SETTING ALL NAN TO 0 CONFORMING TO XGBOOST\n",
    "        temp_df.fillna(0, inplace=True)\n",
    "\n",
    "        print(\"NUM NANS AFTER FILLNA:\", temp_df.isna().sum())\n",
    "\n",
    "\n",
    "        temp_df['location'] = pd.factorize(temp_df['location'])[0]\n",
    "        \n",
    "        if 'diffuse_rad_1h:J' in df.columns and 'direct_rad_1h:J' in df.columns:\n",
    "            temp_df['total_rad_1h:J'] = df['diffuse_rad_1h:J'] + df['direct_rad_1h:J']\n",
    "\n",
    "        temp_df.dropna(axis=0, how=\"all\", subset=[\"total_rad_1h:J\"], inplace=True)\n",
    "\n",
    "        if 'ds' in temp_df.columns:\n",
    "            try:\n",
    "                temp_df['ds'] = pd.to_datetime(temp_df['ds'])\n",
    "                temp_df['hour_of_day'] = temp_df['ds'].dt.hour\n",
    "                temp_df['month'] = temp_df['ds'].dt.month\n",
    "            except:\n",
    "                print(\"Error converting ds to datetime\")\n",
    "\n",
    "        if 'y' in temp_df.columns:\n",
    "            temp_df.dropna(axis=0, how=\"all\", subset=[\"y\"], inplace=True)\n",
    "\n",
    "        selected_cols = [\n",
    "            'location', \n",
    "            'total_rad_1h:J',\n",
    "            'hour_of_day', \n",
    "            'month', \n",
    "            'clear_sky_energy_1h:J', \n",
    "            'sun_elevation:d'\n",
    "        ]\n",
    "\n",
    "        if 'y' in df.columns:\n",
    "            selected_cols.append('y')\n",
    "\n",
    "\n",
    "        return temp_df[selected_cols]\n",
    "\n",
    "    def train(self, df):\n",
    "        df = self.preprocess(df)\n",
    "        features = [col for col in df.columns if col != 'y']\n",
    "        X = df[features]\n",
    "        y = df['y']\n",
    "        self.model = xgb.XGBRegressor()\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, df):\n",
    "        df = self.preprocess(df)\n",
    "        features = [col for col in df.columns if col != 'y']\n",
    "        X = df[features]\n",
    "        y_preds = self.model.predict(X)\n",
    "        return pd.DataFrame({'y_pred': y_preds})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. XG Boost Henrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostHenrik(MetaModel):\n",
    "    \n",
    "    def __init__(self, features = None):\n",
    "        super().__init__(\"XGBoost Henrik\")\n",
    "        self.features = []\n",
    "        if features is not None:\n",
    "            self.features = features  # Use the provided features if not None\n",
    "        else:\n",
    "            self.features.extend(['is_estimated', 'sample_importance',\n",
    "                              'dayofyear',\n",
    "                             'hour',\n",
    "                            'total_rad_1h:J',\n",
    "        'absolute_humidity_2m:gm3',\n",
    "       'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J',\n",
    "       'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx',\n",
    "       'dew_point_2m:K', 'effective_cloud_cover:p', 'elevation:m',\n",
    "       'fresh_snow_12h:cm', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm',\n",
    "       'fresh_snow_3h:cm', 'fresh_snow_6h:cm',\n",
    "       'is_in_shadow:idx', 'msl_pressure:hPa', 'precip_5min:mm',\n",
    "       'precip_type_5min:idx', 'pressure_100m:hPa', 'pressure_50m:hPa',\n",
    "       'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "       'sfc_pressure:hPa', 'snow_density:kgm3', 'snow_depth:cm',\n",
    "       'snow_drift:idx', 'snow_melt_10min:mm', 'snow_water:kgm2',\n",
    "       'sun_azimuth:d', 'sun_elevation:d', 'super_cooled_liquid_water:kgm2',\n",
    "       't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "       'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms',\n",
    "       'wind_speed_w_1000hPa:ms'])\n",
    "            \n",
    "        \n",
    "    def preprocess(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        temp_df = df.copy()\n",
    "\n",
    "        has_target = 'y' in temp_df.columns        \n",
    "        \n",
    "        ##################################################################################### \n",
    "        # FEATURE ENGINEERING\n",
    "        #####################################################################################\n",
    "\n",
    "         # Emphasize test start-end: Starting date: 2023-05-01 00:00:00 Ending data 2023-07-03 23:00:00\n",
    "        temp_df['sample_importance'] = 1\n",
    "        temp_df.loc[(temp_df['ds'].dt.month >= 5) & \n",
    "                    (temp_df['ds'].dt.month < 7), 'sample_importance'] = 2\n",
    "        \n",
    "        temp_df.loc[(temp_df['ds'].dt.month == 7) &\n",
    "                    (temp_df['ds'].dt.day <= 4), 'sample_importance'] = 2\n",
    "        \n",
    "        # Add is_estimated parameter\n",
    "        temp_df['is_estimated'] = (temp_df['weather_data_type'] == 'estimated')\n",
    "        temp_df['is_estimated'] = temp_df['is_estimated'].astype(int)\n",
    "\n",
    "        temp_df['total_rad_1h:J'] = temp_df['diffuse_rad_1h:J'] + temp_df['direct_rad_1h:J']    \n",
    "        \n",
    "        # Extracting hour-of-day and month, and making them cyclical\n",
    "        temp_df['hour'] = temp_df['ds'].dt.hour\n",
    "        temp_df['hour'] = (np.sin(2 * np.pi * (temp_df['hour'] - 4)/ 24) + 1) / 2\n",
    "\n",
    "        temp_df['dayofyear'] = temp_df['ds'].dt.day_of_year\n",
    "        temp_df['dayofyear'] = np.sin(2 * np.pi * (temp_df['dayofyear'] - 80)/ 365)\n",
    "   \n",
    "        # SETTING NAN TO 0 CONFORMING TO XGBOOST\n",
    "        temp_df.fillna(0, inplace=True)\n",
    "\n",
    "        #####################################################################################\n",
    "\n",
    "        # DROPPING UNEEEDED FEATURES\n",
    "        if(has_target):\n",
    "            features_w_y = self.features + ['y']\n",
    "            temp_df = temp_df[features_w_y]\n",
    "\n",
    "        else:\n",
    "            temp_df = temp_df[self.features]\n",
    "\n",
    "        return temp_df\n",
    "\n",
    "    def train(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        temp_df = self.preprocess(df)\n",
    "\n",
    "        # Separate features and target\n",
    "        X = temp_df.drop('y', axis=1, inplace=False).copy().values\n",
    "        y = temp_df['y'].copy().values\n",
    "\n",
    "        # Train test split\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "\n",
    "        params = {\n",
    "            'objective': \"reg:absoluteerror\",\n",
    "            'eta': 0.25,\n",
    "            'max_depth': 8,\n",
    "            'lambda': 1\n",
    "        }\n",
    "\n",
    "        # Setup XGB\n",
    "        self.model = xgb.XGBRegressor(**params)\n",
    "\n",
    "        self.model.fit(\n",
    "            X,\n",
    "            y,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        df = self.preprocess(df)\n",
    "\n",
    "        features = [col for col in df.columns if col != 'y']\n",
    "        X = df[features].values\n",
    "        y_preds = self.model.predict(X)\n",
    "\n",
    "        # Set all negative predictions to 0\n",
    "        y_preds = np.maximum(y_preds, 0)\n",
    "\n",
    "        out_df = pd.DataFrame(data={'y_pred': y_preds})\n",
    "\n",
    "        return out_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Tree Composite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost_henrik import XGBoostHenrik\n",
    "from catboost_henrik import CatBoostHenrik\n",
    "from lightgbm_henrik import LightBGMHenrik\n",
    "\n",
    "class TreeCompositeHenrik(MetaModel):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"TreeComposite Henrik\")\n",
    "        self.features = []\n",
    "        \n",
    "        self.features.extend(['month',\n",
    "                             'hour',\n",
    "                            'total_rad_1h:J',\n",
    "        'absolute_humidity_2m:gm3',\n",
    "       'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J',\n",
    "       'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx',\n",
    "       'dew_point_2m:K', 'effective_cloud_cover:p', 'elevation:m',\n",
    "       'fresh_snow_12h:cm', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm',\n",
    "       'fresh_snow_3h:cm', 'fresh_snow_6h:cm',\n",
    "       'is_in_shadow:idx', 'msl_pressure:hPa', 'precip_5min:mm',\n",
    "       'precip_type_5min:idx', 'pressure_100m:hPa', 'pressure_50m:hPa',\n",
    "       'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "       'sfc_pressure:hPa', 'snow_density:kgm3', 'snow_depth:cm',\n",
    "       'snow_drift:idx', 'snow_melt_10min:mm', 'snow_water:kgm2',\n",
    "       'sun_azimuth:d', 'sun_elevation:d', 'super_cooled_liquid_water:kgm2',\n",
    "       't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "       'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms',\n",
    "       'wind_speed_w_1000hPa:ms'])\n",
    "                                    \n",
    "        \n",
    "    def preprocess(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        temp_df = df.copy()\n",
    "\n",
    "        has_target = 'y' in temp_df.columns        \n",
    "        \n",
    "        ##################################################################################### \n",
    "        # FEATURE ENGINEERING\n",
    "        #####################################################################################\n",
    "\n",
    "        temp_df['total_rad_1h:J'] = temp_df['diffuse_rad_1h:J'] + temp_df['direct_rad_1h:J']    \n",
    "        \n",
    "        # Extracting hour-of-day and month, and making them cyclical\n",
    "        temp_df['hour'] = temp_df['ds'].dt.hour\n",
    "        ml.utils.map_hour_to_seasonal(temp_df, 'hour')\n",
    "\n",
    "        temp_df['month'] = temp_df['ds'].dt.month\n",
    "        ml.utils.map_month_to_seasonal(temp_df, 'month')\n",
    "   \n",
    "        # SETTING NAN TO 0 CONFORMING TO XGBOOST\n",
    "        temp_df.fillna(0, inplace=True)\n",
    "\n",
    "        #####################################################################################\n",
    "\n",
    "        # DROPPING UNEEEDED FEATURES\n",
    "        if(has_target):\n",
    "            features_w_y = self.features + ['y']\n",
    "            temp_df = temp_df[features_w_y]\n",
    "\n",
    "        else:\n",
    "            temp_df = temp_df[self.features]\n",
    "\n",
    "        return temp_df\n",
    "\n",
    "    def train(self, df: MetaModel):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        self.models = {\n",
    "            \"XGBoost Henrik\": XGBoostHenrik(),\n",
    "            \"CatBoost Henrik\": CatBoostHenrik(),\n",
    "            \"LightGBM Henrik\": LightBGMHenrik()\n",
    "        }\n",
    "\n",
    "        for key in self.models:\n",
    "            self.models[key].train(df)\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        all_preds = None\n",
    "\n",
    "        out_df = None\n",
    "\n",
    "        for key in self.models:\n",
    "            y_pred = self.models[key].predict(df)['y_pred']\n",
    "            if(all_preds is None):\n",
    "                all_preds = pd.DataFrame(y_pred)\n",
    "            else:\n",
    "                all_preds[key] = y_pred.values\n",
    "\n",
    "            \n",
    "        avg_series = all_preds.mean(axis=1)\n",
    "\n",
    "        print(\"The different models produced the following predictions:\")\n",
    "        print(all_preds)\n",
    "        print(\"Averages\")\n",
    "        print(avg_series)\n",
    "\n",
    "        return pd.DataFrame(avg_series, columns=['y_pred'])\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. CatBoost Henrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatBoostHenrik(MetaModel):\n",
    "    \n",
    "    def __init__(self, features=None):\n",
    "        super().__init__(\"CatBoost Henrik\")\n",
    "        if(features):\n",
    "            self.features = features\n",
    "        else:\n",
    "            self.features = []\n",
    "            self.features.extend(['sample_importance',\n",
    "                                'dayofyear',\n",
    "                                'hour',\n",
    "                                'total_rad_1h:J',\n",
    "            'absolute_humidity_2m:gm3',\n",
    "            'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J',\n",
    "            'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx',\n",
    "            'dew_point_2m:K', 'effective_cloud_cover:p', 'elevation:m',\n",
    "            'fresh_snow_12h:cm', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm',\n",
    "            'fresh_snow_3h:cm', 'fresh_snow_6h:cm',\n",
    "            'is_in_shadow:idx', 'msl_pressure:hPa', 'precip_5min:mm',\n",
    "            'precip_type_5min:idx', 'pressure_100m:hPa', 'pressure_50m:hPa',\n",
    "            'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "            'sfc_pressure:hPa', 'snow_density:kgm3', 'snow_depth:cm',\n",
    "            'snow_drift:idx', 'snow_melt_10min:mm', 'snow_water:kgm2',\n",
    "            'sun_azimuth:d', 'sun_elevation:d', 'super_cooled_liquid_water:kgm2',\n",
    "            't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "            'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms',\n",
    "            'wind_speed_w_1000hPa:ms'])\n",
    "                           \n",
    "    def preprocess(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        temp_df = df.copy()\n",
    "\n",
    "        has_target = 'y' in temp_df.columns        \n",
    "        \n",
    "        ##################################################################################### \n",
    "        # FEATURE ENGINEERING\n",
    "        #####################################################################################\n",
    "\n",
    "         # Emphasize test start-end: Starting date: 2023-05-01 00:00:00 Ending data 2023-07-03 23:00:00\n",
    "        temp_df['sample_importance'] = 1\n",
    "        temp_df.loc[(temp_df['ds'].dt.month >= 5) & \n",
    "                    (temp_df['ds'].dt.month < 7), 'sample_importance'] = 2\n",
    "        \n",
    "        temp_df.loc[(temp_df['ds'].dt.month == 7) &\n",
    "                    (temp_df['ds'].dt.day <= 4), 'sample_importance'] = 2\n",
    "        \n",
    "        # Add is_estimated parameter\n",
    "        temp_df['is_estimated'] = (temp_df['weather_data_type'] == 'estimated')\n",
    "        temp_df['is_estimated'] = temp_df['is_estimated'].astype(int)\n",
    "\n",
    "        temp_df['total_rad_1h:J'] = temp_df['diffuse_rad_1h:J'] + temp_df['direct_rad_1h:J']    \n",
    "        \n",
    "        # Extracting hour-of-day and month, and making them cyclical\n",
    "        temp_df['hour'] = temp_df['ds'].dt.hour\n",
    "        temp_df['hour'] = (np.sin(2 * np.pi * (temp_df['hour'] - 4)/ 24) + 1) / 2\n",
    "\n",
    "        temp_df['month'] = temp_df['ds'].dt.month\n",
    "        temp_df['month'] = (np.sin(2 * np.pi * (temp_df['month'])/ 12) + 1) / 2\n",
    "\n",
    "\n",
    "        temp_df['dayofyear'] = temp_df['ds'].dt.day_of_year\n",
    "        temp_df['dayofyear'] = np.sin(2 * np.pi * (temp_df['dayofyear'] - 80)/ 365)\n",
    "   \n",
    "        # SETTING NAN TO 0 CONFORMING TO XGBOOST\n",
    "        temp_df.fillna(0, inplace=True)\n",
    "\n",
    "        #####################################################################################\n",
    "\n",
    "        # DROPPING UNEEEDED FEATURES\n",
    "        if(has_target):\n",
    "            features_w_y = self.features + ['y']\n",
    "            temp_df = temp_df[features_w_y]\n",
    "\n",
    "        else:\n",
    "            temp_df = temp_df[self.features]\n",
    "\n",
    "        return temp_df\n",
    "\n",
    "    def train(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        temp_df = self.preprocess(df)\n",
    "\n",
    "        # Separate features and target\n",
    "        X = temp_df.drop('y', axis=1, inplace=False).copy().values\n",
    "        y = temp_df['y'].copy().values\n",
    "\n",
    "        # Train test split\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "\n",
    "        params = {\n",
    "            'objective': \"MAE\",\n",
    "            'learning_rate': 0.04,\n",
    "            'depth': 9,\n",
    "            'iterations': 7000,\n",
    "            'logging_level': 'Silent'\n",
    "        }\n",
    "\n",
    "        \n",
    "        # Setup XGB\n",
    "        self.model = cb.CatBoostRegressor(**params)\n",
    "\n",
    "              \n",
    "        self.model.fit(\n",
    "             X,\n",
    "             y,\n",
    "             verbose=True,\n",
    "             sample_weight=temp_df['sample_importance']\n",
    "        )\n",
    "        \n",
    "    def predict(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        df = self.preprocess(df)\n",
    "\n",
    "        features = [col for col in df.columns if col != 'y']\n",
    "        X = df[features].values\n",
    "        y_preds = self.model.predict(X)\n",
    "\n",
    "        # Set all negative predictions to 0\n",
    "        y_preds = np.maximum(y_preds, 0)\n",
    "\n",
    "        out_df = pd.DataFrame(data={'y_pred': y_preds})\n",
    "\n",
    "        return out_df\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. LightGBM Henrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightBGMHenrik(MetaModel):\n",
    "    \n",
    "    def __init__(self, features=None):\n",
    "        super().__init__(\"LightBGM Henrik\")\n",
    "\n",
    "        self.features = []\n",
    "\n",
    "        if features:\n",
    "            self.features = features\n",
    "        \n",
    "        else:\n",
    "            self.features.extend(['sample_importance',\n",
    "                                'dayofyear',\n",
    "                                'hour',\n",
    "                                'total_rad_1h:J',\n",
    "                                'is_day:idx',\n",
    "            'absolute_humidity_2m:gm3',\n",
    "            'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J',\n",
    "            'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx',\n",
    "            'dew_point_2m:K', 'effective_cloud_cover:p', 'elevation:m',\n",
    "            'fresh_snow_12h:cm', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm',\n",
    "            'fresh_snow_3h:cm', 'fresh_snow_6h:cm',\n",
    "            'is_in_shadow:idx', 'msl_pressure:hPa', 'precip_5min:mm',\n",
    "            'precip_type_5min:idx', 'pressure_100m:hPa', 'pressure_50m:hPa',\n",
    "            'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "            'sfc_pressure:hPa', 'snow_density:kgm3', 'snow_depth:cm',\n",
    "            'snow_drift:idx', 'snow_melt_10min:mm', 'snow_water:kgm2',\n",
    "            'sun_azimuth:d', 'sun_elevation:d', 'super_cooled_liquid_water:kgm2',\n",
    "            't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "            'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms',\n",
    "            'wind_speed_w_1000hPa:ms'])\n",
    "                                      \n",
    "        \n",
    "    def preprocess(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        temp_df = df.copy()\n",
    "\n",
    "        has_target = 'y' in temp_df.columns\n",
    "        \n",
    "        ##################################################################################### \n",
    "        # FEATURE ENGINEERING\n",
    "        #####################################################################################\n",
    "\n",
    "         # Emphasize test start-end: Starting date: 2023-05-01 00:00:00 Ending data 2023-07-03 23:00:00\n",
    "        temp_df['sample_importance'] = 1\n",
    "        temp_df.loc[(temp_df['ds'].dt.month >= 5) & \n",
    "                    (temp_df['ds'].dt.month < 7), 'sample_importance'] = 2\n",
    "        \n",
    "        temp_df.loc[(temp_df['ds'].dt.month == 7) &\n",
    "                    (temp_df['ds'].dt.day <= 4), 'sample_importance'] = 2\n",
    "        \n",
    "        # Add is_estimated parameter\n",
    "        temp_df['is_estimated'] = (temp_df['weather_data_type'] == 'estimated')\n",
    "        temp_df['is_estimated'] = temp_df['is_estimated'].astype(int)\n",
    "\n",
    "        temp_df['total_rad_1h:J'] = temp_df['diffuse_rad_1h:J'] + temp_df['direct_rad_1h:J']    \n",
    "        \n",
    "        # Extracting hour-of-day and month, and making them cyclical\n",
    "        temp_df['hour'] = temp_df['ds'].dt.hour\n",
    "        temp_df['hour'] = (np.sin(2 * np.pi * (temp_df['hour'] - 4)/ 24) + 1) / 2\n",
    "\n",
    "        temp_df['month'] = temp_df['ds'].dt.month\n",
    "        temp_df['month'] = (np.sin(2 * np.pi * (temp_df['month'])/ 12) + 1) / 2\n",
    "\n",
    "        temp_df['dayofyear'] = temp_df['ds'].dt.day_of_year\n",
    "        temp_df['dayofyear'] = np.sin(2 * np.pi * (temp_df['dayofyear'] - 80)/ 365)\n",
    "   \n",
    "        # SETTING NAN TO 0 CONFORMING TO LGBM\n",
    "        temp_df.fillna(0, inplace=True)\n",
    "    \n",
    "        #####################################################################################\n",
    "\n",
    "        # DROPPING UNEEEDED FEATURES\n",
    "        if(has_target):\n",
    "            features_w_y = self.features + ['y']\n",
    "            temp_df = temp_df[features_w_y]\n",
    "\n",
    "        else:\n",
    "            temp_df = temp_df[self.features]\n",
    "\n",
    "        return temp_df\n",
    "\n",
    "    def train(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        temp_df = self.preprocess(df)\n",
    "\n",
    "        # Separate features and target\n",
    "        sample_importance = temp_df['sample_importance']\n",
    "        X = temp_df.drop(['y','sample_importance'], axis=1, inplace=False).copy().values\n",
    "        y = temp_df['y'].copy().values\n",
    "\n",
    "        # Train test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9, random_state=42)\n",
    "\n",
    "        num_iterations = 2000\n",
    "        early_stopping_percentage = 0.1\n",
    "\n",
    "        params = {\n",
    "            'objective': \"mae\",\n",
    "            'learning_rate': 0.1,\n",
    "            'num_leaves': 128,\n",
    "            'zero_as_missing': True,\n",
    "            'num_iterations': num_iterations\n",
    "        }\n",
    "\n",
    "\n",
    "        # Setup XGB\n",
    "        self.model = lgbm.LGBMRegressor(**params)\n",
    "\n",
    "        self.model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set = (X_test, y_test),\n",
    "            #sample_weight = sample_importance,\n",
    "            eval_metric = 'mae',\n",
    "            early_stopping_rounds = round(num_iterations * early_stopping_percentage)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        df = self.preprocess(df)\n",
    "\n",
    "        features = [col for col in df.columns if col not in ['y', 'sample_importance']]\n",
    "        X = df[features].values\n",
    "        y_preds = self.model.predict(X)\n",
    "\n",
    "        # Set all negative predictions to 0\n",
    "        y_preds = np.maximum(y_preds, 0)\n",
    "\n",
    "        out_df = pd.DataFrame(data={'y_pred': y_preds})\n",
    "\n",
    "        return out_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. AutoML H20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADD MODEL CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Autogluon Henrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoGluonHenrik(MetaModel):\n",
    "    \n",
    "    def __init__(self, time_limit=None, excluded_models: list = []):\n",
    "        super().__init__(\"AutoGluon Henrik\")\n",
    "\n",
    "        self.time_limit = time_limit\n",
    "        self.excluded_models = excluded_models\n",
    "\n",
    "        self.features = []\n",
    "        \n",
    "        self.features.extend(['sample_importance',\n",
    "                            'total_rad_1h:J',\n",
    "        'absolute_humidity_2m:gm3',\n",
    "       'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J',\n",
    "       'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx',\n",
    "       'dew_point_2m:K', 'effective_cloud_cover:p', 'elevation:m',\n",
    "       'fresh_snow_12h:cm', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm',\n",
    "       'fresh_snow_3h:cm', 'fresh_snow_6h:cm',\n",
    "       'is_in_shadow:idx', 'msl_pressure:hPa', 'precip_5min:mm',\n",
    "       'precip_type_5min:idx', 'pressure_100m:hPa', 'pressure_50m:hPa',\n",
    "       'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "       'sfc_pressure:hPa', 'snow_density:kgm3', 'snow_depth:cm',\n",
    "       'snow_drift:idx', 'snow_melt_10min:mm', 'snow_water:kgm2',\n",
    "       'sun_azimuth:d', 'sun_elevation:d', 'super_cooled_liquid_water:kgm2',\n",
    "       't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "       'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms',\n",
    "       'wind_speed_w_1000hPa:ms'])\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.features.extend(['total_rad_1h:J', 'month', 'hour', 'sun_elevation:d', 'effective_cloud_cover:p'])\n",
    "        \"\"\"\n",
    "\n",
    "                                  \n",
    "        \n",
    "\n",
    "    def preprocess(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        temp_df = df.copy()\n",
    "\n",
    "        has_target = 'y' in df.columns\n",
    "\n",
    "        if has_target:\n",
    "            temp_df = temp_df[temp_df['y'].notna()]\n",
    "\n",
    "        ##################################################################################### \n",
    "        # FEATURE ENGINEERING\n",
    "        #####################################################################################\n",
    "\n",
    "        # Emphasize test start-end: Starting date: 2023-05-01 00:00:00 Ending data 2023-07-03 23:00:00\n",
    "        temp_df['sample_importance'] = 1\n",
    "        temp_df.loc[(temp_df['ds'].dt.month >= 5) & \n",
    "                    (temp_df['ds'].dt.month < 7), 'sample_importance'] = 2\n",
    "        \n",
    "        temp_df.loc[(temp_df['ds'].dt.month == 7) &\n",
    "                    (temp_df['ds'].dt.day <= 4), 'sample_importance'] = 2\n",
    "        \n",
    "        # Add is_estimated parameter\n",
    "        temp_df['is_estimated'] = (temp_df['weather_data_type'] == 'estimated')\n",
    "        temp_df['is_estimated'] = temp_df['is_estimated'].astype(int)\n",
    "\n",
    "        temp_df['total_rad_1h:J'] = temp_df['diffuse_rad_1h:J'] + temp_df['direct_rad_1h:J']    \n",
    "        \n",
    "        # Extracting hour-of-day and month, and making them cyclical\n",
    "        temp_df['hour'] = temp_df['ds'].dt.hour\n",
    "        temp_df['hour'] = (np.sin(2 * np.pi * (temp_df['hour'] - 4)/ 24) + 1) / 2\n",
    "\n",
    "        temp_df['dayofyear'] = temp_df['ds'].dt.day_of_year\n",
    "        temp_df['dayofyear'] = np.sin(2 * np.pi * (temp_df['dayofyear'] - 80)/ 365)\n",
    "   \n",
    "        # SETTING NAN TO 0 CONFORMING TO XGBOOST\n",
    "        temp_df.fillna(0, inplace=True)\n",
    "\n",
    "        #####################################################################################\n",
    "        if(has_target):\n",
    "            \n",
    "            return temp_df[self.features + ['y']]\n",
    "        else:\n",
    "            return temp_df[self.features]\n",
    "\n",
    "    def train(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        temp_df = self.preprocess(df)\n",
    "\n",
    "\n",
    "        train_data = TabularDataset(temp_df)\n",
    "\n",
    "        self.model = TabularPredictor(\n",
    "            label='y',\n",
    "            eval_metric='mean_absolute_error',\n",
    "            problem_type='regression',\n",
    "            sample_weight='sample_importance',\n",
    "            weight_evaluation=True\n",
    "        ).fit(train_data,\n",
    "              time_limit = self.time_limit,\n",
    "              excluded_model_types = self.excluded_models,\n",
    "              presets=['good_quality']\n",
    "              )\n",
    "\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        df = self.preprocess(df)\n",
    "\n",
    "        features = [col for col in df.columns if col != 'y']\n",
    "        X = df[features]\n",
    "\n",
    "        y_preds = self.model.predict(X)\n",
    "        print(\"AUTOGLUON MODEL OVERVIEW:\")\n",
    "        print(self.model.get_model_names())\n",
    "       \n",
    "\n",
    "        out_df = pd.DataFrame(data={'y_pred': y_preds})\n",
    "\n",
    "        return out_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. LGBM Composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm_henrik import LightBGMHenrik\n",
    "\n",
    "class LGBMCompositeHenrik(MetaModel):\n",
    "    \n",
    "    def __init__(self, num_models=10):\n",
    "        super().__init__(\"LGBMComposite Henrik\")\n",
    "\n",
    "        self.num_models = num_models\n",
    "        self.common_features = ['sample_importance', 'is_estimated','dayofyear',\n",
    "                                'is_day:idx',\n",
    "                             'hour', 'month',\n",
    "                            'total_rad_1h:J',\n",
    "                            'sun_elevation:d',\n",
    "                            'sun_azimuth:d',\n",
    "                            'is_in_shadow:idx',\n",
    "                            'effective_cloud_cover:p']\n",
    "        \n",
    "        self.random_features = ['absolute_humidity_2m:gm3',\n",
    "                                'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J',\n",
    "                                'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx',\n",
    "                                'dew_point_2m:K', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W',\n",
    "                                'direct_rad_1h:J', 'fresh_snow_3h:cm',\n",
    "                                'precip_5min:mm','precip_type_5min:idx', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "                                'sfc_pressure:hPa','snow_water:kgm2',\n",
    "                                'super_cooled_liquid_water:kgm2',\n",
    "                                't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "                                'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms']\n",
    "                           \n",
    "        \n",
    "    def preprocess(self, df: pd.DataFrame):\n",
    "\n",
    "        return df\n",
    "\n",
    "    def train(self, df: pd.DataFrame):\n",
    "        df = self.preprocess(df)\n",
    "        num_rand_features = round(len(self.random_features) * 0.85)\n",
    "\n",
    "        features = dict()\n",
    "\n",
    "        self.models = dict()\n",
    "\n",
    "        for i in range(self.num_models):\n",
    "            temp_rand_features = random.sample(self.random_features, num_rand_features)\n",
    "            features[i] = self.common_features + temp_rand_features\n",
    "            self.models[f'LGBM_{i} Henrik'] = LightBGMHenrik(features = features[i])\n",
    "\n",
    "            print(f'LGBM_{i} Henrik')\n",
    "\n",
    "        for key in self.models:\n",
    "            self.models[key].train(df)\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        all_preds = None\n",
    "\n",
    "        for key in self.models:\n",
    "            y_pred = self.models[key].predict(df)['y_pred']\n",
    "            if(all_preds is None):\n",
    "                all_preds = pd.DataFrame(y_pred)\n",
    "            else:\n",
    "                all_preds[key] = y_pred.values\n",
    "\n",
    "            \n",
    "        avg_series = all_preds.mean(axis=1)\n",
    "\n",
    "        avg_series = np.maximum(avg_series, 0)\n",
    "\n",
    "        return pd.DataFrame(avg_series, columns=['y_pred'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Composite Composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_here_hahah.cat_composite import CatCompositeHenrik\n",
    "from xg_boost_composite import XGBoostComposite\n",
    "from autogluon_henrik import AutoGluonHenrik, AutoGluonJacob\n",
    "from catboost_henrik import CatBoostHenrik\n",
    "\n",
    "class CompositeCompositeHenrik(MetaModel):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"CompositeComposite Henrik\")\n",
    "\n",
    "        self.common_features = ['dayofyear',\n",
    "                             'hour',\n",
    "                            'total_rad_1h:J',\n",
    "                            'sun_elevation:d',\n",
    "                            'sun_azimuth:d',\n",
    "                            'is_in_shadow:idx',\n",
    "                            'effective_cloud_cover:p']\n",
    "        \n",
    "        self.random_features = ['absolute_humidity_2m:gm3',\n",
    "                            'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J',\n",
    "                            'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx',\n",
    "                            'dew_point_2m:K', 'elevation:m',\n",
    "                            'fresh_snow_12h:cm', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm',\n",
    "                            'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'msl_pressure:hPa', 'precip_5min:mm',\n",
    "                            'precip_type_5min:idx', 'pressure_100m:hPa', 'pressure_50m:hPa',\n",
    "                            'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "                            'sfc_pressure:hPa', 'snow_density:kgm3', 'snow_depth:cm',\n",
    "                            'snow_drift:idx', 'snow_melt_10min:mm', 'snow_water:kgm2', 'super_cooled_liquid_water:kgm2',\n",
    "                            't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "                            'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms',\n",
    "                            'wind_speed_w_1000hPa:ms']\n",
    "                                    \n",
    "        \n",
    "    def preprocess(self, df: pd.DataFrame):\n",
    "\n",
    "        return df.copy()\n",
    "\n",
    "    def train(self, df: MetaModel):\n",
    "        \n",
    "        # Excluded AutoGluon models (https://auto.gluon.ai/0.4.0/api/autogluon.predictor.html)\n",
    "        excluded_ag_models = None ### NOT GOOD!!!!!!! DONT EXCLUDEEEE\n",
    "\n",
    "        df = df.copy()\n",
    "\n",
    "        df.drop(df[(df['ds'].dt.month > 9) & (df['ds'].dt.month < 4)].index, inplace = True)\n",
    "\n",
    "        self.models = {\n",
    "            #\"XGBoost Composite\": XGBoostComposite(),\n",
    "            #\"AutoGluon 5min\": AutoGluonHenrik(time_limit=60*30, excluded_models=excluded_ag_models),\n",
    "            \"AutoGluonJacob 10min\": AutoGluonJacob(time_limit=60*10),\n",
    "            \"CatComposite x20\": CatCompositeHenrik(num_models=20)\n",
    "        }\n",
    "\n",
    "        for key in self.models:\n",
    "            self.models[key].train(df)\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        all_preds = None\n",
    "\n",
    "        out_df = None\n",
    "\n",
    "        for key in self.models:\n",
    "            y_pred = self.models[key].predict(df)['y_pred']\n",
    "            if(all_preds is None):\n",
    "                all_preds = pd.DataFrame(y_pred)\n",
    "            else:\n",
    "                all_preds[key] = y_pred.values\n",
    "\n",
    "            \n",
    "        avg_series = all_preds.mean(axis=1)\n",
    "\n",
    "        print(\"The different models produced the following predictions:\")\n",
    "        print(all_preds)\n",
    "\n",
    "        avg_series = np.maximum(avg_series, 0)\n",
    "\n",
    "        return pd.DataFrame(avg_series, columns=['y_pred'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. XG Boost Composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostComposite(MetaModel):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"XGBoost Composite\")\n",
    "\n",
    "        self.common_features = ['is_estimated','dayofyear',\n",
    "                             'hour',\n",
    "                            'total_rad_1h:J',\n",
    "                            'sun_elevation:d',\n",
    "                            'sun_azimuth:d',\n",
    "                            'is_in_shadow:idx',\n",
    "                            'effective_cloud_cover:p']\n",
    "        \n",
    "        self.random_features = ['absolute_humidity_2m:gm3',\n",
    "                            'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J',\n",
    "                            'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx',\n",
    "                            'dew_point_2m:K', 'elevation:m',\n",
    "                            'fresh_snow_12h:cm', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm',\n",
    "                            'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'msl_pressure:hPa', 'precip_5min:mm',\n",
    "                            'precip_type_5min:idx', 'pressure_100m:hPa', 'pressure_50m:hPa',\n",
    "                            'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "                            'sfc_pressure:hPa', 'snow_density:kgm3', 'snow_depth:cm',\n",
    "                            'snow_drift:idx', 'snow_melt_10min:mm', 'snow_water:kgm2', 'super_cooled_liquid_water:kgm2',\n",
    "                            't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "                            'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms',\n",
    "                            'wind_speed_w_1000hPa:ms']\n",
    "                    \n",
    "        \n",
    "    def preprocess(self, df: pd.DataFrame):\n",
    "        return df.copy()\n",
    "\n",
    "    \n",
    "    def train(self, df: pd.DataFrame, use_meta_learner=True):\n",
    "        num_models = 30\n",
    "        num_rand_features = round(len(self.random_features) * 0.8)  \n",
    "        df = df.copy()\n",
    "        df['month'] = df['ds'].dt.month\n",
    "\n",
    "        meta_train_df = df[(df['month'] == 5) | (df['month'] == 6) | (df['month'] == 7)].sample(frac=0.5)\n",
    "        print(\"Meta-train % of full DF\", len(meta_train_df)/len(df))\n",
    "        train_df = df.loc[~df.index.isin(meta_train_df)]\n",
    "\n",
    "        features = dict()\n",
    "        self.models = dict()\n",
    "\n",
    "        for i in range(num_models):\n",
    "            temp_rand_features = random.sample(self.random_features, num_rand_features)\n",
    "            features[i] = self.common_features + temp_rand_features\n",
    "            self.models[f'XGBOOST_{i}'] = XGBoostHenrik(features = features[i])\n",
    "\n",
    "        for key in self.models:\n",
    "            print(\"Training model\", key)\n",
    "            self.models[key].train(train_df)\n",
    "        \n",
    "        if (use_meta_learner):                        \n",
    "            y_preds = self.predict(meta_train_df, meta_training=True)\n",
    "\n",
    "            self.meta_learner = LinearRegression(fit_intercept=False, positive=True)\n",
    "            self.meta_learner.fit(y_preds, meta_train_df['y'])\n",
    "\n",
    "            # Adjust weights so all are non-zero and positive\n",
    "            \"\"\"\n",
    "            new_coefficients = np.copy(self.meta_learner.coef_) + 1/num_models\n",
    "            new_coefficients = new_coefficients / new_coefficients.min()\n",
    "            new_coefficients = new_coefficients / new_coefficients.sum()\n",
    "            self.meta_learner.coef_ = new_coefficients\n",
    "            \"\"\"\n",
    "\n",
    "            self.meta_learner.coef_ = np.full(shape = self.meta_learner.coef_.shape, fill_value = 1/num_models)\n",
    "\n",
    "            print(self.meta_learner.coef_)\n",
    "    \n",
    "    def predict(self, df, meta_training = False):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        all_preds = None\n",
    "        out_df = None\n",
    "        for key in self.models:\n",
    "            y_pred = self.models[key].predict(df)['y_pred']\n",
    "            if(all_preds is None):\n",
    "                all_preds = pd.DataFrame(y_pred)\n",
    "            else:\n",
    "                all_preds[key] = y_pred.values\n",
    "\n",
    "        if (meta_training):\n",
    "            print(\"RETURNING ALL_PREDS\")\n",
    "            return pd.DataFrame(all_preds)\n",
    "\n",
    "        #print(\"THIS HAS GONE TOO FAR!\")\n",
    "\n",
    "        # Use meta-learner to calculate final output\n",
    "\n",
    "        out_np = self.meta_learner.predict(all_preds)\n",
    "        #print(out_np)\n",
    "\n",
    "        \"\"\"\n",
    "        out_np = all_preds.mean(axis=1)\n",
    "\n",
    "        print(\"The different models produced the following predictions:\")\n",
    "        print(out_np)\n",
    "\n",
    "        out_np = np.maximum(out_np, 0)\n",
    "        \"\"\"\n",
    "        return pd.DataFrame(out_np, columns=['y_pred'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Autogluon Jacob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autogluon'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mautogluon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtabular\u001b[39;00m \u001b[39mimport\u001b[39;00m TabularDataset, TabularPredictor\n\u001b[0;32m      3\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mAutoGluonJacob\u001b[39;00m(MetaModel):\n\u001b[0;32m      5\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, time_limit\u001b[39m=\u001b[39m\u001b[39m60\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'autogluon'"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "class AutoGluonJacob(MetaModel):\n",
    "    \n",
    "    def __init__(self, time_limit=60*3):\n",
    "        super().__init__(\"AutoGluon Jacob\")\n",
    "\n",
    "        self.time_limit = time_limit\n",
    "\n",
    "        # autogluon features\n",
    "        # TabularPredictor (usage : **params_TabularPredictor)\n",
    "        self.params_TabularPredictor = \\\n",
    "            {\n",
    "                'label': 'y',\n",
    "                'problem_type': 'regression', \n",
    "                'eval_metric': 'mean_absolute_error',\n",
    "                'verbosity': 2,\n",
    "            } \n",
    "        # TabularPredictor.fit\n",
    "        self.params_TabularPredictor_fit = \\\n",
    "            {\n",
    "                'time_limit': self.time_limit,\n",
    "                'presets': 'high_quality', # [‘best_quality’, ‘high_quality’, ‘good_quality’, ‘medium_quality’, ‘optimize_for_deployment’, ‘interpretable’, ‘ignore_text’]\n",
    "                #'hyperparameters': 'default',\n",
    "                # 'auto_stack': False,\n",
    "                # 'num_bag_folds': None, # set automatically by auto_stack True\n",
    "                # 'num_bag_sets': None, # set to 20 because of auto_stack\n",
    "                # 'num_stack_levels': None, # set automatically by auto_stack True\n",
    "                'hyperparameter_tune_kwargs': 'auto', # None to disable\n",
    "                # 'refit_full': True,\n",
    "                # 'feature_prune_kwargs': {}, # If None, do not perform feature pruning. If empty dictionary, perform feature pruning with default configurations.\n",
    "            }\n",
    "\n",
    "        self.use_tuning_data = True # 'sample_weight', 'random'\n",
    "        self.use_sample_weight = True\n",
    "\n",
    "        if self.use_sample_weight: # auto_weight a feature that exists\n",
    "            self.params_TabularPredictor['sample_weight'] = 'sample_importance'\n",
    "\n",
    "\n",
    "        self.common_features = ['sample_importance', 'weather_data_type','dayofyear',\n",
    "                                'is_day:idx',\n",
    "                             'hour', 'month',\n",
    "                            'total_rad_1h:J',\n",
    "                            'sun_elevation:d',\n",
    "                            'sun_azimuth:d',\n",
    "                            'is_in_shadow:idx',\n",
    "                            'effective_cloud_cover:p']\n",
    "        \n",
    "        self.random_features = ['absolute_humidity_2m:gm3',\n",
    "                                'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J',\n",
    "                                'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx',\n",
    "                                'dew_point_2m:K', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W',\n",
    "                                'direct_rad_1h:J', 'fresh_snow_3h:cm',\n",
    "                                'precip_5min:mm','precip_type_5min:idx', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "                                'sfc_pressure:hPa','snow_water:kgm2',\n",
    "                                'super_cooled_liquid_water:kgm2',\n",
    "                                't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "                                'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms']\n",
    "        \n",
    "\n",
    "    def preprocess(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        temp_df = df.copy()\n",
    "\n",
    "        temp_df['total_rad_1h:J'] = temp_df['diffuse_rad_1h:J'] + temp_df['direct_rad_1h:J']    \n",
    "        \n",
    "        # Extracting hour-of-day and month, and making them cyclical\n",
    "        temp_df['hour'] = temp_df['ds'].dt.hour\n",
    "        temp_df['hour'] = (np.sin(2 * np.pi * (temp_df['hour'] - 4)/ 24) + 1) / 2\n",
    "\n",
    "        temp_df['dayofyear'] = temp_df['ds'].dt.day_of_year\n",
    "        temp_df['dayofyear'] = np.sin(2 * np.pi * (temp_df['dayofyear'] - 80)/ 365)\n",
    "\n",
    "        # temp_df['year'] = temp_df['ds'].dt.hour\n",
    "        temp_df['month'] = temp_df['ds'].dt.month\n",
    "        # temp_df['day'] = temp_df['ds'].dt.day\n",
    "        # temp_df['dayofweek'] = temp_df['ds'].dt.dayofweek\n",
    "\n",
    "        if self.use_sample_weight:\n",
    "            # Emphasize test start-end: Starting date: 2023-05-01 00:00:00 Ending data 2023-07-03 23:00:00\n",
    "            temp_df['sample_importance'] = 1\n",
    "            temp_df.loc[(temp_df['ds'].dt.month >= 5) & \n",
    "                        (temp_df['ds'].dt.month < 7), 'sample_importance'] = 2\n",
    "            \n",
    "            temp_df.loc[(temp_df['ds'].dt.month == 7) &\n",
    "                         (temp_df['ds'].dt.day <= 4), 'sample_importance'] = 2\n",
    "\n",
    "\n",
    "\n",
    "        return temp_df.drop(columns=['ds'])[self.common_features + self.random_features + (['y'] if 'y' in temp_df else [])].copy()\n",
    "\n",
    "    def train(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        print(\"Training JacobGluon\")\n",
    "        temp_df = self.preprocess(df)\n",
    "\n",
    "        # temp_df.drop(df[(df['ds'].dt.month > 9) & (df['ds'].dt.month < 4)].index, inplace = True)\n",
    "\n",
    "\n",
    "        if self.use_tuning_data:\n",
    "\n",
    "            tuning_data = temp_df[(temp_df['month'] == 5) | (temp_df['month'] == 6)].sample(frac=0.5, random_state=42)\n",
    "            train_data = TabularDataset(temp_df[~temp_df.isin(tuning_data.to_dict(orient='list')).all(1)])\n",
    "\n",
    "            self.model = TabularPredictor(**self.params_TabularPredictor).fit(train_data, tuning_data=tuning_data, use_bag_holdout=True, **self.params_TabularPredictor_fit)\n",
    "        else:\n",
    "            train_data = TabularDataset(temp_df)\n",
    "\n",
    "            self.model = TabularPredictor(**self.params_TabularPredictor).fit(train_data, **self.params_TabularPredictor_fit)\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        df = self.preprocess(df)\n",
    "\n",
    "        features = [col for col in df.columns if col != 'y']\n",
    "        X = df[features]\n",
    "\n",
    "        y_preds = self.model.predict(X)\n",
    "        print(\"AUTOGLUON MODEL OVERVIEW:\")\n",
    "        print(self.model.leaderboard())       \n",
    "\n",
    "        out_df = pd.DataFrame(data={'y_pred': y_preds})\n",
    "\n",
    "        return out_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. CatBoost Composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost_henrik import CatBoostHenrik\n",
    "\n",
    "class CatCompositeHenrik(MetaModel):\n",
    "    \n",
    "    def __init__(self, num_models=10):\n",
    "        super().__init__(\"CatComposite Henrik\")\n",
    "\n",
    "        self.num_models = num_models\n",
    "        # self.common_features = ['sample_importance', 'is_estimated','dayofyear',\n",
    "        #                         'is_day:idx',\n",
    "        #                      'hour', 'month',\n",
    "        #                     'total_rad_1h:J',\n",
    "        #                     'sun_elevation:d',\n",
    "        #                     'sun_azimuth:d',\n",
    "        #                     'is_in_shadow:idx',\n",
    "        #                     'effective_cloud_cover:p']\n",
    "        \n",
    "        # self.random_features = ['absolute_humidity_2m:gm3',\n",
    "        #                     'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J',\n",
    "        #                     'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx',\n",
    "        #                     'dew_point_2m:K',\n",
    "        #                     'fresh_snow_12h:cm', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm',\n",
    "        #                     'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'msl_pressure:hPa', 'precip_5min:mm',\n",
    "        #                     'precip_type_5min:idx', 'pressure_100m:hPa', 'pressure_50m:hPa',\n",
    "        #                     'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "        #                     'sfc_pressure:hPa', 'snow_depth:cm',\n",
    "        #                     'snow_water:kgm2', 'super_cooled_liquid_water:kgm2',\n",
    "        #                     't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "        #                     'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms']\n",
    "        \n",
    "        self.common_features = ['sample_importance', 'is_estimated','dayofyear',\n",
    "                                'is_day:idx',\n",
    "                             'hour', 'month',\n",
    "                            'total_rad_1h:J',\n",
    "                            'sun_elevation:d',\n",
    "                            'sun_azimuth:d',\n",
    "                            'is_in_shadow:idx',\n",
    "                            'effective_cloud_cover:p']\n",
    "        \n",
    "        self.random_features = ['absolute_humidity_2m:gm3',\n",
    "                                'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J',\n",
    "                                'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx',\n",
    "                                'dew_point_2m:K', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W',\n",
    "                                'direct_rad_1h:J', 'fresh_snow_3h:cm',\n",
    "                                'precip_5min:mm','precip_type_5min:idx', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "                                'sfc_pressure:hPa','snow_water:kgm2',\n",
    "                                'super_cooled_liquid_water:kgm2',\n",
    "                                't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "                                'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms']\n",
    "        \"\"\"\n",
    "        # FEATURES WITHOUT SNOW\n",
    "        self.random_features = ['absolute_humidity_2m:gm3',\n",
    "                            'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J',\n",
    "                            'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx',\n",
    "                            'dew_point_2m:K', 'elevation:m',\n",
    "                            'msl_pressure:hPa', 'precip_5min:mm',\n",
    "                            'precip_type_5min:idx', 'pressure_100m:hPa', 'pressure_50m:hPa',\n",
    "                            'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "                            'sfc_pressure:hPa',\n",
    "                            'super_cooled_liquid_water:kgm2',\n",
    "                            't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "                            'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms',\n",
    "                            'wind_speed_w_1000hPa:ms']\n",
    "        \"\"\"         \n",
    "        \n",
    "    def preprocess(self, df: pd.DataFrame):\n",
    "        return df.copy()\n",
    "\n",
    "    \n",
    "    def train(self, df: pd.DataFrame, use_meta_learner=True):\n",
    "        num_models = self.num_models\n",
    "        num_rand_features = round(len(self.random_features) * 1)\n",
    "        df = df.copy()\n",
    "        df['month'] = df['ds'].dt.month\n",
    "\n",
    "        # random_states = [41, 61, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60]#[i for i in range(num_models - 1)]\n",
    "        random_states = [i for i in range(23, 43)]\n",
    "\n",
    "\n",
    "        meta_train_df = df[(df['month'] == 5) | (df['month'] == 6) | (df['month'] == 7)].sample(frac=0.5)\n",
    "        print(\"Meta-train % of full DF\", len(meta_train_df)/len(df))\n",
    "\n",
    "        # DUPLICATE MONTHS WE PREDICT\n",
    "        selected_months = df[(df['month'] == 5) | (df['month'] == 6) | (df['month'] == 7)].copy()\n",
    "        train_df = pd.concat([df, selected_months], ignore_index=True)\n",
    "\n",
    "        features = dict()\n",
    "        self.models = dict()\n",
    "\n",
    "        for i in range(num_models):\n",
    "            temp_rand_features = random.sample(self.random_features, num_rand_features)\n",
    "            features[i] = self.common_features + temp_rand_features\n",
    "            self.models[f'CATBOOST_{i}'] = CatBoostHenrik(features = features[i], random_state=random_states[i])\n",
    "\n",
    "        for key in self.models:\n",
    "            print(\"Training model\", key)\n",
    "            self.models[key].train(train_df)\n",
    "        \n",
    "        \"\"\"\n",
    "        if (use_meta_learner):                        \n",
    "            y_preds = self.predict(meta_train_df, meta_training=True)\n",
    "\n",
    "            self.meta_learner = LinearRegression(fit_intercept=False, positive=True)\n",
    "            self.meta_learner.fit(y_preds, meta_train_df['y'])\n",
    "\n",
    "            # Adjust weights so all are non-zero and positive\n",
    "            new_coefficients = np.copy(self.meta_learner.coef_) + 1/num_models\n",
    "            new_coefficients = new_coefficients / new_coefficients.min()\n",
    "            new_coefficients = new_coefficients / new_coefficients.sum()\n",
    "            self.meta_learner.coef_ = new_coefficients\n",
    "\n",
    "            print(self.meta_learner.coef_)\n",
    "        \"\"\"\n",
    "    \n",
    "    def predict(self, df, meta_training = False):\n",
    "\n",
    "        all_preds = None\n",
    "        out_df = None\n",
    "        for key in self.models:\n",
    "            y_pred = self.models[key].predict(df)['y_pred']\n",
    "            if(all_preds is None):\n",
    "                all_preds = pd.DataFrame(y_pred)\n",
    "            else:\n",
    "                all_preds[key] = y_pred.values\n",
    "\n",
    "        if (meta_training):\n",
    "            print(\"RETURNING ALL_PREDS\")\n",
    "            return pd.DataFrame(all_preds)\n",
    "\n",
    "        \"\"\"\n",
    "        #print(\"THIS HAS GONE TOO FAR!\")\n",
    "\n",
    "        # Use meta-learner to calculate final output (DISABLED)\n",
    "        out_np = self.meta_learner.predict(all_preds)\n",
    "        #print(out_np)\n",
    "        \"\"\"\n",
    "        out_np = all_preds.mean(axis=1)\n",
    "\n",
    "        print(\"The different models produced the following predictions:\")\n",
    "        print(out_np)\n",
    "\n",
    "        out_np = np.maximum(out_np, 0)\n",
    "\n",
    "        return pd.DataFrame(out_np, columns=['y_pred'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 91. FeedForward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNNHenrik(MetaModel):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"Feedforward Neural Network Henrik\")\n",
    "        self.features = []\n",
    "\n",
    "        \"\"\"\n",
    "        self.features.extend(['month',\n",
    "                             'hour',\n",
    "                            'total_rad_1h:J',\n",
    "                            'sun_elevation:d'])\n",
    "        \"\"\"\n",
    "        \n",
    "        self.features.extend(['month',\n",
    "                             'hour',\n",
    "                            'total_rad_1h:J',\n",
    "                            'fresh_snow_12h:cm',\n",
    "                            'snow_water:kgm2',\n",
    "                            'is_day:idx',\n",
    "                            'is_in_shadow:idx',\n",
    "                            'rain_water:kgm2',\n",
    "                            'sun_azimuth:d',\n",
    "                            'sun_elevation:d',\n",
    "                            't_1000hPa:K',\n",
    "                            'dew_or_rime:idx',\n",
    "                            'air_density_2m:kgm3',\n",
    "                            'absolute_humidity_2m:gm3'])\n",
    "        \n",
    "        \"\"\"\n",
    "        self.features.extend(['super_cooled_liquid_water:kgm2',\n",
    "                              'effective_cloud_cover:p', 'elevation:m',\n",
    "                              'fresh_snow_1h:cm', 'fresh_snow_24h:cm',\n",
    "                              'msl_pressure:hPa', 'precip_5min:mm', 'prob_rime:p', \n",
    "                              'relative_humidity_1000hPa:p', 'visibility:m'\n",
    "                              ])\n",
    "        \"\"\"                              \n",
    "        \n",
    "    def preprocess(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        temp_df = df.copy()\n",
    "\n",
    "        has_target = 'y' in temp_df.columns        \n",
    "        \n",
    "        ##################################################################################### \n",
    "        # FEATURE ENGINEERING\n",
    "        #####################################################################################\n",
    "\n",
    "        temp_df['total_rad_1h:J'] = temp_df['diffuse_rad_1h:J'] + temp_df['direct_rad_1h:J']    \n",
    "        \n",
    "        # Extracting hour-of-day and month, and making them cyclical\n",
    "        temp_df['hour'] = temp_df['ds'].dt.hour\n",
    "        ml.utils.map_hour_to_seasonal(temp_df, 'hour')\n",
    "\n",
    "        temp_df['month'] = temp_df['ds'].dt.month\n",
    "        ml.utils.map_month_to_seasonal(temp_df, 'month')\n",
    "\n",
    "        # Make NaN interpolated\n",
    "        ml.utils.interpolate_na(temp_df, self.features)\n",
    "\n",
    "\n",
    "        # Normalize the features\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        temp_df[self.features] = scaler.fit_transform(temp_df[self.features])\n",
    "\n",
    "        #####################################################################################\n",
    "\n",
    "        # DROPPING UNEEEDED FEATURES\n",
    "        if(has_target):\n",
    "            features_w_y = self.features + ['y']\n",
    "            temp_df = temp_df[features_w_y]\n",
    "\n",
    "        else:\n",
    "            temp_df = temp_df[self.features]\n",
    "\n",
    "        return temp_df\n",
    "\n",
    "    def train(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        temp_df = self.preprocess(df)\n",
    "\n",
    "        # Separate features and target\n",
    "        X = temp_df.drop('y', axis=1, inplace=False).copy().values\n",
    "        y = temp_df['y'].copy().values\n",
    "\n",
    "        # Train test split\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "\n",
    "        \n",
    "        print('Num features:', len(self.features))\n",
    "\n",
    "        # Setup XGB\n",
    "        self.model = models.Sequential()\n",
    "        \"\"\"\n",
    "        self.model.add(layers.Dense(input_dim=len(self.features),\n",
    "                       units=len(self.features), \n",
    "                       activation=\"relu\"))\n",
    "        \n",
    "        self.model.add(layers.Dense(input_dim=len(self.features),\n",
    "                       units=32, \n",
    "                       activation=\"relu\"))\n",
    "        \n",
    "        self.model.add(layers.Dense(input_dim=32,\n",
    "                       units=32, \n",
    "                       activation=\"relu\"))\n",
    "\n",
    "\n",
    "        self.model.add(layers.Dense(input_dim=32,\n",
    "                       units=32, \n",
    "                       activation=\"relu\"))\n",
    "        \n",
    "        self.model.add(layers.Dense(input_dim=32,\n",
    "                       units=32, \n",
    "                       activation=\"relu\"))\n",
    "        \n",
    "\n",
    "        self.model.add(layers.Dense(input_dim=32,\n",
    "                       units=16, \n",
    "                       activation=\"relu\"))\n",
    "        \"\"\"\n",
    "\n",
    "        # Params\n",
    "        learning_rate = 0.35\n",
    "        \n",
    "\n",
    "        # add the output layer\n",
    "        self.model.add(layers.Dense(input_dim=len(self.features),\n",
    "                            units=1,\n",
    "                            activation='relu'))\n",
    "\n",
    "        # define our loss function and optimizer\n",
    "        self.model.compile(loss='mean_absolute_error',\n",
    "                    # Adam is a kind of gradient descent\n",
    "                    optimizer=optimizers.SGD(learning_rate=learning_rate))\n",
    "\n",
    "\n",
    "        self.model.fit(\n",
    "            X,\n",
    "            y,\n",
    "            verbose=True,\n",
    "            epochs=200,\n",
    "            batch_size=32\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        df = self.preprocess(df)\n",
    "\n",
    "        features = [col for col in df.columns if col != 'y']\n",
    "        X = df[features].values\n",
    "        y_preds = self.model.predict(X)\n",
    "\n",
    "        # Set all negative predictions to 0\n",
    "        y_preds = np.maximum(y_preds, 0)\n",
    "\n",
    "        # Make y_preds 1D\n",
    "        y_preds = np.ravel(y_preds)\n",
    "\n",
    "        out_df = pd.DataFrame(data={'y_pred': y_preds})\n",
    "\n",
    "        return out_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 92. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(MetaModel):\n",
    "    def __init__(self, window_len):\n",
    "        super().__init__(\"LSTM\")\n",
    "\n",
    "        self.keep_columns = ['total_rad_1h:J',\n",
    "                        'fresh_snow_12h:cm',\n",
    "                        'snow_water:kgm2',\n",
    "                        'is_day:idx',\n",
    "                        'is_in_shadow:idx',\n",
    "                        'rain_water:kgm2',\n",
    "                        'sun_azimuth:d',\n",
    "                        'sun_elevation:d',\n",
    "                        't_1000hPa:K',\n",
    "                        'dew_or_rime:idx',\n",
    "                        'air_density_2m:kgm3',\n",
    "                        'absolute_humidity_2m:gm3',\n",
    "                        'y'\n",
    "        ]\n",
    "\n",
    "        self.window_len = window_len\n",
    "        self.batch_size = 1\n",
    "        self.num_features = None\n",
    "        self.epochs = 1\n",
    "        self.scaler = None\n",
    "    \n",
    "    def test(self, df, n_splits=5):\n",
    "        print(f\"Testing {self.model_name}\")\n",
    "        column_names = df.columns.tolist()\n",
    "        if 'y' not in column_names:\n",
    "            raise Exception(f\"Missing observed y in columns. Available are {column_names}\")\n",
    "\n",
    "        # This is unecessary because we already clean it when calling train\n",
    "        #df_cleaned = self.preprocess(df)\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "        MAE_values = []\n",
    "\n",
    "        for train_index, test_index in tscv.split(df):\n",
    "\n",
    "            train_partition = df.iloc[train_index]\n",
    "            valid_partition = df.iloc[test_index]\n",
    "\n",
    "            # Needs to be adjusted due to window length\n",
    "            adj_valid_partition = pd.concat([train_partition[-self.window_len:], valid_partition], ignore_index=True)\n",
    "\n",
    "            self.train(train_partition)\n",
    "            predictions = self.predict(adj_valid_partition)\n",
    "            \n",
    "            y_true = valid_partition['y']\n",
    "            y_pred = predictions['y_pred']\n",
    "\n",
    "            MAE = mean_absolute_error(y_true, y_pred)\n",
    "            MAE_values.append(MAE)\n",
    "\n",
    "            print(f'Train-Test ratio:{len(train_partition)/len(valid_partition)} achieved MAE {MAE}')\n",
    "\n",
    "        return MAE_values\n",
    "        \n",
    "    def preprocess(self, df, has_target_col=False):\n",
    "        temp_df = df.copy()\n",
    "\n",
    "        temp_df['total_rad_1h:J'] = df['diffuse_rad_1h:J'] + df['direct_rad_1h:J']\n",
    "\n",
    "        # Only keep the columns in keep_columns that df actually has\n",
    "        keep_columns_exist = [col for col in self.keep_columns if col in temp_df.columns]\n",
    "        temp_df = temp_df[keep_columns_exist]\n",
    "        \n",
    "        # Clean out NaNs/NONEs -- May need to remove for more than just total_rad_1h:J\n",
    "        #temp_df['total_rad_1h:J'].fillna(df['total_rad_1h:J'].interpolate().cummax())\n",
    "        #temp_df = temp_df.dropna(axis=0, how=\"all\", subset=\"total_rad_1h:J\")\n",
    "\n",
    "        if(has_target_col):\n",
    "            temp_df['y'].fillna(df['y'].interpolate().cummax())\n",
    "            # temp_df = temp_df.dropna(axis=0, how=\"all\", subset=\"y\")\n",
    "\n",
    "        # Min-max scale all columns so all values in [0, 1]\n",
    "        self.scaler = StandardScaler()\n",
    "        temp_np = self.scaler.fit_transform(temp_df)\n",
    "\n",
    "        # Convert to time-series of given window-lengths \n",
    "        # (Sequence_length is how long each time-window is, sequence_stride is how long the window shifts forward in time for each sequence)\n",
    "        if(has_target_col):\n",
    "            self.num_features = temp_np.shape[1] - 1\n",
    "\n",
    "            features = temp_np[:, : self.num_features]\n",
    "            targets = temp_np[:, self.num_features]\n",
    "\n",
    "            temp_df = TimeseriesGenerator(data=features, targets=targets, length=self.window_len, shuffle=False, batch_size=self.batch_size)\n",
    "        \n",
    "        else:\n",
    "            self.num_features = temp_np.shape[1]\n",
    "\n",
    "            features = temp_np[:, : self.num_features]\n",
    "            dummy_targets = np.zeros(features.shape)\n",
    "\n",
    "            temp_df = TimeseriesGenerator(data=features, targets=dummy_targets, length=self.window_len, shuffle=False, batch_size=self.batch_size)\n",
    "        \n",
    "        # Returns a generator (keras datatype)\n",
    "        return temp_df\n",
    "\n",
    "    def train(self, df):\n",
    "        # Make train-test split\n",
    "        train_end = round(len(df) * 0.7)\n",
    "        # test_df = df.iloc[train_end:]\n",
    "\n",
    "        train_generator = self.preprocess(df, has_target_col = True)\n",
    "        # test_generator = self.preprocess(test_df, has_target_col=True)\n",
    "\n",
    "        # Design the model\n",
    "        self.model = tf.keras.Sequential()\n",
    "\n",
    "        self.model.add(tf.keras.layers.LSTM(64, input_shape=(self.window_len, self.num_features), return_sequences=True))\n",
    "        self.model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n",
    "\n",
    "        self.model.add(tf.keras.layers.LSTM(64, return_sequences=True))\n",
    "        self.model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n",
    "\n",
    "        self.model.add(tf.keras.layers.Dropout(0.3))\n",
    "        self.model.add(tf.keras.layers.LSTM(32, return_sequences=False))\n",
    "\n",
    "        self.model.add(tf.keras.layers.Dropout(0.3))\n",
    "        self.model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "        # Add pruning measures\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode='min')\n",
    "        self.model.compile(loss=tf.losses.MeanSquaredError(), optimizer=tf.optimizers.Adam(), metrics=tf.metrics.MeanAbsoluteError())\n",
    "\n",
    "        # Train the model\n",
    "        history = self.model.fit(train_generator, epochs=self.epochs, shuffle=False, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "    def predict(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        output_generator = self.preprocess(df, ('y' in df.columns))\n",
    "        model_out = self.model.predict(output_generator)\n",
    "\n",
    "        print(model_out)\n",
    "        print(model_out.shape)\n",
    "\n",
    "        ### RESCALE BACK TO ORIGINAL SIZE\n",
    "        temp_scaler = StandardScaler()\n",
    "        index_of_y = self.keep_columns.index(\"y\")\n",
    "        temp_scaler.mean_, temp_scaler.scale_ = self.scaler.mean_[index_of_y], self.scaler.scale_[index_of_y]\n",
    "\n",
    "        scaled_model_out = temp_scaler.inverse_transform(model_out)\n",
    "        print(scaled_model_out)\n",
    "\n",
    "        data = {'y_pred': scaled_model_out.flatten()}\n",
    "        out_df = pd.DataFrame(data)\n",
    "\n",
    "        if(len(df) != len(out_df) + self.window_len):\n",
    "            print(\"INPUT DF WAS CLEANED S.T. INDEXES WONT MATCH UP\")\n",
    "            print(\"Original df:\", len(df), \"Output pred_df\", len(out_df))\n",
    "\n",
    "        return out_df\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 93. PCA Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCARegression(MetaModel):\n",
    "    \n",
    "    def __init__(self, pca_dimensions: int = 6):\n",
    "        super().__init__(\"PCA Regression\")\n",
    "        self.keep_columns = ['ds',\n",
    "                            'total_rad_1h:J',\n",
    "                            'fresh_snow_12h:cm',\n",
    "                            'snow_water:kgm2',\n",
    "                            'is_day:idx',\n",
    "                            'is_in_shadow:idx',\n",
    "                            'rain_water:kgm2',\n",
    "                            'sun_azimuth:d',\n",
    "                            'sun_elevation:d',\n",
    "                            't_1000hPa:K',\n",
    "                            'dew_or_rime:idx',\n",
    "                            'air_density_2m:kgm3',\n",
    "                            'absolute_humidity_2m:gm3',\n",
    "                            'y'\n",
    "        ]\n",
    "\n",
    "        self.pca_dimensions = pca_dimensions\n",
    "    \n",
    "    def interpolate_na(self, df, cols):\n",
    "        for col in cols:\n",
    "            df[col].ffill(inplace=True)\n",
    "            df[col].bfill(inplace=True)\n",
    "\n",
    "            df[col].fillna(df[col].interpolate().cummax(), inplace=True)            \n",
    "        \n",
    "    def preprocess(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        temp_df = df.copy()\n",
    "\n",
    "        has_target_col = 'y' in temp_df.columns\n",
    "\n",
    "        temp_df['total_rad_1h:J'] = temp_df['diffuse_rad_1h:J'] + temp_df['direct_rad_1h:J']\n",
    "\n",
    "\n",
    "        self.keep_columns_exist = [col for col in self.keep_columns if col in temp_df.columns]\n",
    "        self.interpolate_na(temp_df, self.keep_columns_exist)\n",
    "        temp_df = temp_df[self.keep_columns_exist]\n",
    "\n",
    "        # Add hour of day\n",
    "        temp_df['hour'] = temp_df['ds'].dt.hour\n",
    "        ml.utils.map_hour_to_seasonal(temp_df, 'hour')\n",
    "        temp_df = temp_df.drop('ds', axis=1)\n",
    "\n",
    "        num_features = len(self.keep_columns) - 1\n",
    "\n",
    "        if(has_target_col):\n",
    "            temp_df['y'].fillna(df['y'].interpolate().cummax(), inplace=True)\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        temp_np = self.scaler.fit_transform(temp_df)\n",
    "        features = temp_np[:, : num_features]\n",
    "\n",
    "        pca = PCA(n_components=self.pca_dimensions)\n",
    "        pca_features = pca.fit_transform(features)\n",
    "\n",
    "        if(has_target_col):\n",
    "            target = temp_np[:, num_features]\n",
    "            pca_w_target = np.column_stack((pca_features, target))\n",
    "\n",
    "            col_names = [f\"PCA{i}\" for i in range(self.pca_dimensions)] + ['y']\n",
    "\n",
    "            temp_df = pd.DataFrame(data=pca_w_target, columns=col_names)\n",
    "        else:\n",
    "            col_names = [f\"PCA{i}\" for i in range(self.pca_dimensions)] + ['y']\n",
    "            temp_df = pd.DataFrame(data=pca_features, columns=col_names)\n",
    "\n",
    "        return temp_df\n",
    "\n",
    "    def train(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        df = self.preprocess(df)\n",
    "\n",
    "        PCA_features = df.drop('y', axis=1, inplace=False)\n",
    "\n",
    "        self.model = LinearRegression()\n",
    "\n",
    "        self.model.fit(PCA_features, df['y'])\n",
    "\n",
    "\n",
    "    def predict(self, df):\n",
    "        df = self.preprocess(df)\n",
    "\n",
    "        if('y' in df.columns):\n",
    "            df = df.drop('y', axis=1, inplace=False)\n",
    "\n",
    "        y_preds = self.model.predict(df)\n",
    "        y_preds_arr = np.array(y_preds)\n",
    "\n",
    "        out_df = pd.DataFrame(y_preds_arr, columns=[\"y_pred\"]) \n",
    "\n",
    "        # SCALE BACK OUTPUT\n",
    "        temp_scaler = StandardScaler()\n",
    "        index_of_y = self.keep_columns.index(\"y\")\n",
    "        temp_scaler.mean_, temp_scaler.scale_ = self.scaler.mean_[index_of_y], self.scaler.scale_[index_of_y]\n",
    "\n",
    "        scaled_model_out = temp_scaler.inverse_transform(out_df)\n",
    "        out_df = pd.DataFrame(data=scaled_model_out, columns=['y_pred'])\n",
    "\n",
    "        out_df['y_pred'] = out_df['y_pred'].apply(lambda a : max(0, a))\n",
    "\n",
    "        #print(\"NUM NEGATIVE OUTPUTS\", (out_df['y_pred'] < 0).sum(), \"AVERAGE NEG VAL\", (out_df[out_df['y_pred'] < 0].mean()))\n",
    "\n",
    "        return out_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 94. TreeComposite with MetaLearner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeCompositeMetaLearnerHenrik(MetaModel):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"TreeCompositeMetaLearner Henrik\")\n",
    "        self.features = []\n",
    "        \n",
    "        self.features.extend(['month',\n",
    "                             'hour',\n",
    "                            'total_rad_1h:J',\n",
    "        'absolute_humidity_2m:gm3',\n",
    "       'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J',\n",
    "       'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx',\n",
    "       'dew_point_2m:K', 'effective_cloud_cover:p', 'elevation:m',\n",
    "       'fresh_snow_12h:cm', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm',\n",
    "       'fresh_snow_3h:cm', 'fresh_snow_6h:cm',\n",
    "       'is_in_shadow:idx', 'msl_pressure:hPa', 'precip_5min:mm',\n",
    "       'precip_type_5min:idx', 'pressure_100m:hPa', 'pressure_50m:hPa',\n",
    "       'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "       'sfc_pressure:hPa', 'snow_density:kgm3', 'snow_depth:cm',\n",
    "       'snow_drift:idx', 'snow_melt_10min:mm', 'snow_water:kgm2',\n",
    "       'sun_azimuth:d', 'sun_elevation:d', 'super_cooled_liquid_water:kgm2',\n",
    "       't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "       'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms',\n",
    "       'wind_speed_w_1000hPa:ms'])\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.features.extend(['month',\n",
    "                             'hour',\n",
    "                            'total_rad_1h:J',\n",
    "                            'fresh_snow_12h:cm',\n",
    "                            'snow_water:kgm2',\n",
    "                            'is_day:idx',\n",
    "                            'is_in_shadow:idx',\n",
    "                            'rain_water:kgm2',\n",
    "                            'sun_azimuth:d',\n",
    "                            'sun_elevation:d',\n",
    "                            't_1000hPa:K',\n",
    "                            'dew_or_rime:idx',\n",
    "                            'air_density_2m:kgm3',\n",
    "                            'absolute_humidity_2m:gm3'])\n",
    "        \n",
    "\n",
    "        self.features.extend(['super_cooled_liquid_water:kgm2',\n",
    "                              'effective_cloud_cover:p', 'elevation:m',\n",
    "                              'fresh_snow_1h:cm', 'fresh_snow_24h:cm',\n",
    "                              'msl_pressure:hPa', 'precip_5min:mm', 'prob_rime:p', \n",
    "                              'relative_humidity_1000hPa:p', 'visibility:m'\n",
    "                              ])\n",
    "        \"\"\"                              \n",
    "        \n",
    "    def preprocess(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        temp_df = df.copy()\n",
    "\n",
    "        has_target = 'y' in temp_df.columns        \n",
    "        \n",
    "        ##################################################################################### \n",
    "        # FEATURE ENGINEERING\n",
    "        #####################################################################################\n",
    "\n",
    "        temp_df['total_rad_1h:J'] = temp_df['diffuse_rad_1h:J'] + temp_df['direct_rad_1h:J']    \n",
    "        \n",
    "        # Extracting hour-of-day and month, and making them cyclical\n",
    "        temp_df['hour'] = temp_df['ds'].dt.hour\n",
    "        ml.utils.map_hour_to_seasonal(temp_df, 'hour')\n",
    "\n",
    "        temp_df['month'] = temp_df['ds'].dt.month\n",
    "        ml.utils.map_month_to_seasonal(temp_df, 'month')\n",
    "   \n",
    "        # SETTING NAN TO 0 CONFORMING TO XGBOOST\n",
    "        temp_df.fillna(0, inplace=True)\n",
    "\n",
    "        #####################################################################################\n",
    "\n",
    "        # DROPPING UNEEEDED FEATURES\n",
    "        if(has_target):\n",
    "            features_w_y = self.features + ['y']\n",
    "            temp_df = temp_df[features_w_y]\n",
    "\n",
    "        else:\n",
    "            temp_df = temp_df[self.features]\n",
    "\n",
    "        return temp_df\n",
    "\n",
    "    def train(self, df: MetaModel):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        self.models = {\n",
    "            \"XGBoost Henrik\": XGBoostHenrik(),\n",
    "            \"LightGBM Henrik\": LightBGMHenrik()\n",
    "        }\n",
    "\n",
    "        for key in self.models:\n",
    "            self.models[key].train(df)\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        all_preds = None\n",
    "\n",
    "        out_df = None\n",
    "\n",
    "        for key in self.models:\n",
    "            y_pred = self.models[key].predict(df)['y_pred']\n",
    "            if(all_preds is None):\n",
    "                all_preds = pd.DataFrame(y_pred)\n",
    "            else:\n",
    "                all_preds[key] = y_pred.values\n",
    "\n",
    "            \n",
    "        avg_series = all_preds.mean(axis=1)\n",
    "\n",
    "        print(\"The different models produced the following predictions:\")\n",
    "        print(all_preds)\n",
    "        print(\"Averages\")\n",
    "        print(avg_series)\n",
    "\n",
    "        return pd.DataFrame(avg_series, columns=['y_pred'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
