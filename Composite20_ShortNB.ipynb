{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDT4173 - Machine Learning\n",
    "Names: Jacob Alexander Worsøe, Henrik August Søntvedt, Simen Øygarden Burgos\n",
    "\n",
    "Student IDs: 544553, 543969, 543917\n",
    "\n",
    "Team name: 5tabekk\n",
    "\n",
    "Team number: 5\n",
    "\n",
    "## Long Notebook \n",
    "\n",
    "We worked with a system where we created a python library containing helper functions. These have been placed below and adapted to the notebook format. This library also contained a MetaModel class that we created to more easily test and keep track of our models. This is why our models are written into classes.\n",
    "\n",
    "#### Data location\n",
    "\n",
    "We kept the data in a folder titled data as follows\n",
    " - data\n",
    "    - A\n",
    "    - B\n",
    "    - C\n",
    "    my_first_submission.csv\n",
    "    read_files.ipynb\n",
    "    Readme.md\n",
    "    sample_submission.csv\n",
    "    test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import statistics\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Temporarily suppress FutureWarning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "from keras import models, layers, optimizers, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "import lightgbm as lgbm\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.utils import timeseries_dataset_from_array\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "from prophet import Prophet\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from itertools import combinations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data fetching and preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_training():\n",
    "    \"\"\" gets full training data (duh) \"\"\"\n",
    "\n",
    "    train_a = pd.read_parquet(data_dir + 'A/train_targets.parquet')\n",
    "    train_b = pd.read_parquet(data_dir + 'B/train_targets.parquet')\n",
    "    train_c = pd.read_parquet(data_dir + 'C/train_targets.parquet')\n",
    "\n",
    "    X_train_estimated_a = pd.read_parquet(data_dir + 'A/X_train_estimated.parquet')\n",
    "    X_train_estimated_b = pd.read_parquet(data_dir + 'B/X_train_estimated.parquet')\n",
    "    X_train_estimated_c = pd.read_parquet(data_dir + 'C/X_train_estimated.parquet')\n",
    "\n",
    "    X_train_observed_a = pd.read_parquet(data_dir + 'A/X_train_observed.parquet')\n",
    "    X_train_observed_b = pd.read_parquet(data_dir + 'B/X_train_observed.parquet')\n",
    "    X_train_observed_c = pd.read_parquet(data_dir + 'C/X_train_observed.parquet')\n",
    "\n",
    "    ret = pd.DataFrame()\n",
    "    ret.set_index(pd.MultiIndex(levels=[[],[]], codes=[[],[]], names=['location', 'datetime']), inplace=True)\n",
    "    ret.columns = pd.MultiIndex(levels=[[],[], []], codes=[[],[], []], names=['feature_type', 'minutes', 'feature_name'])\n",
    "\n",
    "\n",
    "    # estimated data\n",
    "    data_dict = {'A': X_train_estimated_a, 'B': X_train_estimated_b, 'C': X_train_estimated_c}\n",
    "\n",
    "    for loc in data_dict:\n",
    "        out_temp = pd.DataFrame(data_dict[loc].date_forecast.dt.floor('H').unique(), columns=['date_forecast'])\n",
    "        out_temp.set_index('date_forecast', inplace=True)\n",
    "        out_temp.columns = pd.MultiIndex.from_product([[], [], out_temp.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "\n",
    "        for m in [0, 15, 30, 45]:\n",
    "            temp = data_dict[loc][data_dict[loc].date_forecast.dt.minute == m].copy()\n",
    "            temp.set_index(temp.date_forecast.dt.floor('H'), inplace=True)\n",
    "            temp.drop(columns=['date_calc', 'date_forecast'], inplace=True)\n",
    "            temp.columns = pd.MultiIndex.from_product([['estimated'], [m], temp.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "            out_temp = out_temp.merge(temp,left_index=True, right_index=True, how='outer')\n",
    "\n",
    "        out_temp.set_index(pd.MultiIndex.from_product([[loc], out_temp.index], names=['location', 'datetime']), inplace=True)\n",
    "        ret = pd.concat([ret, out_temp])\n",
    "\n",
    "    # observed data\n",
    "    data_dict = {'A': X_train_observed_a, 'B': X_train_observed_b, 'C': X_train_observed_c}\n",
    "\n",
    "    for loc in data_dict:\n",
    "        out_temp = pd.DataFrame(data_dict[loc].date_forecast.dt.floor('H').unique(), columns=['date_forecast'])\n",
    "        out_temp.set_index('date_forecast', inplace=True)\n",
    "        out_temp.columns = pd.MultiIndex.from_product([[], [], out_temp.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "\n",
    "        for m in [0, 15, 30, 45]:\n",
    "            temp = data_dict[loc][data_dict[loc].date_forecast.dt.minute == m].copy()\n",
    "            temp.set_index(temp.date_forecast.dt.floor('H'), inplace=True)\n",
    "            temp.drop(columns=['date_forecast'], inplace=True)\n",
    "            temp.columns = pd.MultiIndex.from_product([['observed'], [m], temp.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "            out_temp = out_temp.merge(temp,left_index=True, right_index=True, how='outer')\n",
    "\n",
    "        out_temp.set_index(pd.MultiIndex.from_product([[loc], out_temp.index], names=['location', 'datetime']), inplace=True)\n",
    "        ret = pd.concat([ret, out_temp])\n",
    "        \n",
    "\n",
    "\n",
    "    # # train data\n",
    "    data_dict = {'B': train_b, 'C': train_c} # 'A': train_a, \n",
    "\n",
    "    out_temp = train_a.dropna().copy()\n",
    "    out_temp.rename(columns={'time': 'datetime'}, inplace=True)\n",
    "    out_temp.set_index('datetime', inplace=True)\n",
    "    out_temp.columns = pd.MultiIndex.from_product([['y'], ['NA'], out_temp.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "\n",
    "    out_temp.set_index(pd.MultiIndex.from_product([['A'], out_temp.index], names=['location', 'datetime']), inplace=True)\n",
    "\n",
    "    for loc in data_dict:\n",
    "\n",
    "        out_temp2 = data_dict[loc].dropna().copy()\n",
    "        out_temp2.rename(columns={'time': 'datetime'}, inplace=True)\n",
    "        out_temp2.set_index('datetime', inplace=True)\n",
    "        out_temp2.columns = pd.MultiIndex.from_product([['y'], ['NA'], out_temp2.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "\n",
    "        out_temp2.set_index(pd.MultiIndex.from_product([[loc], out_temp2.index], names=['location', 'datetime']), inplace=True)\n",
    "\n",
    "        out_temp = pd.concat([out_temp, out_temp2])\n",
    "\n",
    "        # ret = ret.merge(out_temp, left_index=True, right_index=True, how='outer')\n",
    "        # ret = pd.concat([ret, out_temp])\n",
    "        \n",
    "\n",
    "    ret = pd.merge(out_temp, ret, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_training_groupby_mean():\n",
    "    return get_training().groupby(level = [0,2], axis = 1).mean()[['y', 'estimated', 'observed']]\n",
    "\n",
    "\n",
    "def get_training_flattened():\n",
    "    df = get_training_groupby_mean()\n",
    "\n",
    "    df['estimated', 'weather_data_type'] = np.nan\n",
    "\n",
    "    df['estimated', 'weather_data_type'] = np.where(df['observed', 'absolute_humidity_2m:gm3'].notna(),'observed', df['estimated', 'weather_data_type'])\n",
    "    df['estimated', 'weather_data_type'] = np.where(df['estimated', 'absolute_humidity_2m:gm3'].notna(),'estimated', df['estimated', 'weather_data_type'])\n",
    "    df.estimated = df.estimated.fillna(df.observed)\n",
    "\n",
    "    df = df.drop(columns=['observed'], level=0)\n",
    "    df.columns = df.columns.droplevel().tolist()\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'datetime': 'ds', 'pv_measurement': 'y'}, inplace=True)\n",
    "    # df['ENG_total_rad'] = df['diffuse_rad_1h:J'] + df['direct_rad_1h:J']\n",
    "    \n",
    "    return df[['location', 'ds', 'y', 'weather_data_type'] + [i for i in df.columns.tolist() if i not in ['location', 'ds', 'y', 'weather_data_type']]].copy()\n",
    "\n",
    "def get_training_cleaned():\n",
    "    df = get_training_flattened()\n",
    "\n",
    "    df = df[\n",
    "            ~((df.y != 0) & \n",
    "            (df.y == df.y.shift(-1)) &\n",
    "            (df.y == df.y.shift(-2)))\n",
    "        ].copy()\n",
    "\n",
    "    df.dropna(axis=0, subset='y', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "### Testing\n",
    "\n",
    "def get_testing():\n",
    "    \"\"\" gets the feature estimates used for the forecast \"\"\"\n",
    "\n",
    "    X_test_estimated_a = pd.read_parquet(data_dir + 'A/X_test_estimated.parquet')\n",
    "    X_test_estimated_b = pd.read_parquet(data_dir + 'B/X_test_estimated.parquet')\n",
    "    X_test_estimated_c = pd.read_parquet(data_dir + 'C/X_test_estimated.parquet')\n",
    "\n",
    "    ret = pd.DataFrame()\n",
    "    ret.set_index(pd.MultiIndex(levels=[[],[]], codes=[[],[]], names=['location', 'datetime']), inplace=True)\n",
    "    ret.columns = pd.MultiIndex(levels=[[],[], []], codes=[[],[], []], names=['feature_type', 'minutes', 'feature_name'])\n",
    "\n",
    "\n",
    "    # estimated data\n",
    "    data_dict = {'A': X_test_estimated_a, 'B': X_test_estimated_b, 'C': X_test_estimated_c}\n",
    "\n",
    "    for loc in data_dict:\n",
    "        out_temp = pd.DataFrame(data_dict[loc].date_forecast.dt.floor('H').unique(), columns=['date_forecast'])\n",
    "        out_temp.set_index('date_forecast', inplace=True)\n",
    "        out_temp.columns = pd.MultiIndex.from_product([[], [], out_temp.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "\n",
    "        for m in [0, 15, 30, 45]:\n",
    "            temp = data_dict[loc][data_dict[loc].date_forecast.dt.minute == m].copy()\n",
    "            temp.set_index(temp.date_forecast.dt.floor('H'), inplace=True)\n",
    "            temp.drop(columns=['date_calc', 'date_forecast'], inplace=True)\n",
    "            temp.columns = pd.MultiIndex.from_product([['estimated'], [m], temp.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "            out_temp = out_temp.merge(temp,left_index=True, right_index=True, how='outer')\n",
    "\n",
    "        out_temp.set_index(pd.MultiIndex.from_product([[loc], out_temp.index], names=['location', 'datetime']), inplace=True)\n",
    "        ret = pd.concat([ret, out_temp])\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_testing_flattened():\n",
    "    df = get_testing()\n",
    "    df = df.groupby(level = [0,2], axis = 1).mean()[['estimated']]\n",
    "    df.columns = df.columns.droplevel().tolist()\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'datetime': 'ds'}, inplace=True)\n",
    "    df['weather_data_type'] = 'estimated'\n",
    "    return df[['location', 'ds', 'weather_data_type'] + [i for i in df.columns.tolist() if i not in ['location', 'ds', 'y', 'weather_data_type']]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_na(df: pd.DataFrame, cols: list, inplace=True):\n",
    "    if inplace:\n",
    "        for col in cols:\n",
    "            df[col].ffill(inplace=True)\n",
    "            df[col].bfill(inplace=True)\n",
    "\n",
    "            df[col].interpolate(inplace=True)   \n",
    "\n",
    "            df[col].fillna(df[col].cummax(), inplace=True)\n",
    "\n",
    "    else:\n",
    "        new_df = None\n",
    "        for col in cols:\n",
    "            new_df = df[col].ffill(inplace=inplace)\n",
    "            new_df = df[col].bfill(inplace=inplace)\n",
    "\n",
    "            new_df = df[col].fillna(df[col].interpolate().cummax(), inplace=inplace)\n",
    "\n",
    "        return new_df\n",
    "\n",
    "\n",
    "def map_hour_to_seasonal(df, hour_col):\n",
    "    mapping = {1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6,\n",
    "               7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12,\n",
    "               13: 11, 14: 10, 15: 9, 16: 8, 17: 7, 18: 6,\n",
    "               19: 5, 20: 4, 21: 3, 22: 2, 23: 1, 24: 0,}\n",
    "    df[hour_col].replace(mapping, inplace=True)\n",
    "\n",
    "def map_month_to_seasonal(df, month_col):\n",
    "    mapping = {1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6,\n",
    "               7: 5, 8: 4, 9: 3, 10: 2, 11: 1, 12: 0}\n",
    "    \n",
    "    df[month_col].replace(mapping, inplace=True)\n",
    "\n",
    "# liste med 'y_pred' \n",
    "\n",
    "def y_pred_to_csv(file_name, df):\n",
    "    df[['y_pred']].reset_index(drop=True).reset_index().rename(columns={'index': 'id', 'y_pred': 'prediction'}).to_csv(module_dir + \"/../submissions/\" + file_name, index=False)\n",
    "\n",
    "def make_submittable(file_name, model = None, model_dict = None):\n",
    "    \"\"\"\n",
    "    model if same model used for all locations\n",
    "    model dict if not\n",
    "\n",
    "    model is instance of model (model = MetaModel())\n",
    "    model dict uses location as key ({\"A\": MetaModel(), \"B\": MetaModel(), \"C\": MetaModel()})\n",
    "    \"\"\"\n",
    "\n",
    "    df = data.get_training_cleaned()\n",
    "    test = data.get_testing_flattened()\n",
    "    ret = pd.DataFrame()\n",
    "    \n",
    "    for location in ['A', 'B', 'C']:\n",
    "        temp_df = df[df['location']==location]\n",
    "        temp_test = test[test['location']==location]\n",
    "\n",
    "        if model is not None:\n",
    "            m = model\n",
    "        elif model_dict is not None:\n",
    "            m = model_dict[location]\n",
    "        else:\n",
    "            raise ValueError(\"no model specified\")\n",
    "    \n",
    "        m.train(temp_df)\n",
    "        fcst = m.predict(temp_test)\n",
    "\n",
    "        ret = pd.concat([ret, fcst])\n",
    "\n",
    "    y_pred_to_csv(file_name, ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MetaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MetaModel(ABC):     \n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "\n",
    "        return\n",
    "\n",
    "    # Denne df-en må ha y som kolonne\n",
    "    def test(self, df: pd.DataFrame, n_splits=5):\n",
    "        \"\"\"\n",
    "            Expanding window cross-validation, df must have y in it for testing against predictions\n",
    "        \"\"\"\n",
    "        print(f\"Testing {self.model_name}\")\n",
    "        column_names = df.columns.tolist()\n",
    "        if 'y' not in column_names:\n",
    "            raise Exception(f\"Missing observed y in columns. Available are {column_names}\")\n",
    "\n",
    "        # This is unecessary because we already clean it when calling train\n",
    "        # drop_y_with_na\n",
    "        df = df.dropna(subset=['y'], inplace=False)\n",
    "\n",
    "        MAE_values = []\n",
    "        MSE_values = []\n",
    "\n",
    "        # tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        kf =KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "        for train_index, test_index in kf.split(df):\n",
    "            train_partition = df.iloc[train_index]\n",
    "            valid_partition = df.iloc[test_index]\n",
    "\n",
    "            self.train(train_partition)\n",
    "            predictions = self.predict(valid_partition)\n",
    "            \n",
    "            y_true = valid_partition['y']\n",
    "            y_pred = predictions['y_pred']\n",
    "\n",
    "            MAE = mean_absolute_error(y_true, y_pred)\n",
    "            MAE_values.append(MAE)\n",
    "\n",
    "            MSE_values.append((y_pred - y_true).mean())\n",
    "\n",
    "            print(f\"Run {len(MAE_values)} MAE =\", MAE)\n",
    "\n",
    "        print(\"Mean Signed Error vals\", MSE_values)\n",
    "        average_mae = statistics.mean(MAE_values)\n",
    "        print(\"MAE Vals: MEAN:\", average_mae, 'ALL:' , MAE_values)\n",
    "        \n",
    "        return MAE_values\n",
    "    \n",
    "    @abstractmethod\n",
    "    def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "            Takes in single-index (datetime as index) df, and returns df with only desired features\n",
    "        \"\"\"    \n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def train(df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "            Runs trained model on on input df, preprocessing the df first and then returns datetime and y_pred\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Naive estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
