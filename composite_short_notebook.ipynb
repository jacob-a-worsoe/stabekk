{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDT4173 - Machine Learning\n",
    "Names: Jacob Alexander Worsøe, Henrik August Søntvedt, Simen Øygarden Burgos\n",
    "\n",
    "Student IDs: 544553, 543969, 543917\n",
    "\n",
    "Team name: 5tabekk\n",
    "\n",
    "Team number: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily suppress FutureWarning\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# In module1.py\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of parent folder\n",
    "current_dir = os.path.abspath(\"\")\n",
    "parent_dir = os.path.join(current_dir, os.pardir)\n",
    "\n",
    "# Add to sys.path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import random\n",
    "import catboost as cb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold, TimeSeriesSplit, train_test_split\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set path to data (we have it in a \"data\" folder with data/A/*.parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### TRAINING DATA ######\n",
    "def get_training():\n",
    "    \"\"\" gets full training data (duh) \"\"\"\n",
    "\n",
    "    train_a = pd.read_parquet(data_dir + 'A/train_targets.parquet')\n",
    "    train_b = pd.read_parquet(data_dir + 'B/train_targets.parquet')\n",
    "    train_c = pd.read_parquet(data_dir + 'C/train_targets.parquet')\n",
    "\n",
    "    X_train_estimated_a = pd.read_parquet(data_dir + 'A/X_train_estimated.parquet')\n",
    "    X_train_estimated_b = pd.read_parquet(data_dir + 'B/X_train_estimated.parquet')\n",
    "    X_train_estimated_c = pd.read_parquet(data_dir + 'C/X_train_estimated.parquet')\n",
    "\n",
    "    X_train_observed_a = pd.read_parquet(data_dir + 'A/X_train_observed.parquet')\n",
    "    X_train_observed_b = pd.read_parquet(data_dir + 'B/X_train_observed.parquet')\n",
    "    X_train_observed_c = pd.read_parquet(data_dir + 'C/X_train_observed.parquet')\n",
    "\n",
    "    ret = pd.DataFrame()\n",
    "    ret.set_index(pd.MultiIndex(levels=[[],[]], codes=[[],[]], names=['location', 'datetime']), inplace=True)\n",
    "    ret.columns = pd.MultiIndex(levels=[[],[], []], codes=[[],[], []], names=['feature_type', 'minutes', 'feature_name'])\n",
    "\n",
    "\n",
    "    # estimated data\n",
    "    data_dict = {'A': X_train_estimated_a, 'B': X_train_estimated_b, 'C': X_train_estimated_c}\n",
    "\n",
    "    for loc in data_dict:\n",
    "        out_temp = pd.DataFrame(data_dict[loc].date_forecast.dt.floor('H').unique(), columns=['date_forecast'])\n",
    "        out_temp.set_index('date_forecast', inplace=True)\n",
    "        out_temp.columns = pd.MultiIndex.from_product([[], [], out_temp.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "\n",
    "        for m in [0, 15, 30, 45]:\n",
    "            temp = data_dict[loc][data_dict[loc].date_forecast.dt.minute == m].copy()\n",
    "            temp.set_index(temp.date_forecast.dt.floor('H'), inplace=True)\n",
    "            temp.drop(columns=['date_calc', 'date_forecast'], inplace=True)\n",
    "            temp.columns = pd.MultiIndex.from_product([['estimated'], [m], temp.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "            out_temp = out_temp.merge(temp,left_index=True, right_index=True, how='outer')\n",
    "\n",
    "        out_temp.set_index(pd.MultiIndex.from_product([[loc], out_temp.index], names=['location', 'datetime']), inplace=True)\n",
    "        ret = pd.concat([ret, out_temp])\n",
    "\n",
    "    # observed data\n",
    "    data_dict = {'A': X_train_observed_a, 'B': X_train_observed_b, 'C': X_train_observed_c}\n",
    "\n",
    "    for loc in data_dict:\n",
    "        out_temp = pd.DataFrame(data_dict[loc].date_forecast.dt.floor('H').unique(), columns=['date_forecast'])\n",
    "        out_temp.set_index('date_forecast', inplace=True)\n",
    "        out_temp.columns = pd.MultiIndex.from_product([[], [], out_temp.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "\n",
    "        for m in [0, 15, 30, 45]:\n",
    "            temp = data_dict[loc][data_dict[loc].date_forecast.dt.minute == m].copy()\n",
    "            temp.set_index(temp.date_forecast.dt.floor('H'), inplace=True)\n",
    "            temp.drop(columns=['date_forecast'], inplace=True)\n",
    "            temp.columns = pd.MultiIndex.from_product([['observed'], [m], temp.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "            out_temp = out_temp.merge(temp,left_index=True, right_index=True, how='outer')\n",
    "\n",
    "        out_temp.set_index(pd.MultiIndex.from_product([[loc], out_temp.index], names=['location', 'datetime']), inplace=True)\n",
    "        ret = pd.concat([ret, out_temp])\n",
    "        \n",
    "\n",
    "\n",
    "    # # train data\n",
    "    data_dict = {'B': train_b, 'C': train_c} # 'A': train_a, \n",
    "\n",
    "    out_temp = train_a.dropna().copy()\n",
    "    out_temp.rename(columns={'time': 'datetime'}, inplace=True)\n",
    "    out_temp.set_index('datetime', inplace=True)\n",
    "    out_temp.columns = pd.MultiIndex.from_product([['y'], ['NA'], out_temp.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "\n",
    "    out_temp.set_index(pd.MultiIndex.from_product([['A'], out_temp.index], names=['location', 'datetime']), inplace=True)\n",
    "\n",
    "    for loc in data_dict:\n",
    "\n",
    "        out_temp2 = data_dict[loc].dropna().copy()\n",
    "        out_temp2.rename(columns={'time': 'datetime'}, inplace=True)\n",
    "        out_temp2.set_index('datetime', inplace=True)\n",
    "        out_temp2.columns = pd.MultiIndex.from_product([['y'], ['NA'], out_temp2.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "\n",
    "        out_temp2.set_index(pd.MultiIndex.from_product([[loc], out_temp2.index], names=['location', 'datetime']), inplace=True)\n",
    "\n",
    "        out_temp = pd.concat([out_temp, out_temp2])\n",
    "\n",
    "        # ret = ret.merge(out_temp, left_index=True, right_index=True, how='outer')\n",
    "        # ret = pd.concat([ret, out_temp])\n",
    "        \n",
    "\n",
    "    ret = pd.merge(out_temp, ret, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "    return ret\n",
    "\n",
    "def get_training_groupby_mean():\n",
    "    return get_training().groupby(level = [0,2], axis = 1).mean()[['y', 'estimated', 'observed']]\n",
    "\n",
    "def get_training_flattened():\n",
    "    df = get_training_groupby_mean()\n",
    "\n",
    "    df['estimated', 'weather_data_type'] = np.nan\n",
    "\n",
    "    df['estimated', 'weather_data_type'] = np.where(df['observed', 'absolute_humidity_2m:gm3'].notna(),'observed', df['estimated', 'weather_data_type'])\n",
    "    df['estimated', 'weather_data_type'] = np.where(df['estimated', 'absolute_humidity_2m:gm3'].notna(),'estimated', df['estimated', 'weather_data_type'])\n",
    "    df.estimated = df.estimated.fillna(df.observed)\n",
    "\n",
    "    df = df.drop(columns=['observed'], level=0)\n",
    "    df.columns = df.columns.droplevel().tolist()\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'datetime': 'ds', 'pv_measurement': 'y'}, inplace=True)\n",
    "    # df['ENG_total_rad'] = df['diffuse_rad_1h:J'] + df['direct_rad_1h:J']\n",
    "    \n",
    "    return df[['location', 'ds', 'y', 'weather_data_type'] + [i for i in df.columns.tolist() if i not in ['location', 'ds', 'y', 'weather_data_type']]].copy()\n",
    "\n",
    "\"\"\"\n",
    "    Based on the EDA we clean the data training data for NAs in pv_measurements, make data the hourly average.\n",
    "\"\"\"\n",
    "def get_training_cleaned():\n",
    "    df = get_training_flattened()\n",
    "\n",
    "    df = df[\n",
    "            ~((df.y != 0) & \n",
    "            (df.y == df.y.shift(-1)) &\n",
    "            (df.y == df.y.shift(-2)))\n",
    "        ].copy()\n",
    "\n",
    "    df.dropna(axis=0, subset='y', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "###### TESTING DATA ######\n",
    "def get_testing():\n",
    "    \"\"\" gets the feature estimates used for the forecast \"\"\"\n",
    "\n",
    "    X_test_estimated_a = pd.read_parquet(data_dir + 'A/X_test_estimated.parquet')\n",
    "    X_test_estimated_b = pd.read_parquet(data_dir + 'B/X_test_estimated.parquet')\n",
    "    X_test_estimated_c = pd.read_parquet(data_dir + 'C/X_test_estimated.parquet')\n",
    "\n",
    "    ret = pd.DataFrame()\n",
    "    ret.set_index(pd.MultiIndex(levels=[[],[]], codes=[[],[]], names=['location', 'datetime']), inplace=True)\n",
    "    ret.columns = pd.MultiIndex(levels=[[],[], []], codes=[[],[], []], names=['feature_type', 'minutes', 'feature_name'])\n",
    "\n",
    "\n",
    "    # estimated data\n",
    "    data_dict = {'A': X_test_estimated_a, 'B': X_test_estimated_b, 'C': X_test_estimated_c}\n",
    "\n",
    "    for loc in data_dict:\n",
    "        out_temp = pd.DataFrame(data_dict[loc].date_forecast.dt.floor('H').unique(), columns=['date_forecast'])\n",
    "        out_temp.set_index('date_forecast', inplace=True)\n",
    "        out_temp.columns = pd.MultiIndex.from_product([[], [], out_temp.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "\n",
    "        for m in [0, 15, 30, 45]:\n",
    "            temp = data_dict[loc][data_dict[loc].date_forecast.dt.minute == m].copy()\n",
    "            temp.set_index(temp.date_forecast.dt.floor('H'), inplace=True)\n",
    "            temp.drop(columns=['date_calc', 'date_forecast'], inplace=True)\n",
    "            temp.columns = pd.MultiIndex.from_product([['estimated'], [m], temp.columns], names=['feature_type', 'minutes', 'feature_name'])\n",
    "            out_temp = out_temp.merge(temp,left_index=True, right_index=True, how='outer')\n",
    "\n",
    "        out_temp.set_index(pd.MultiIndex.from_product([[loc], out_temp.index], names=['location', 'datetime']), inplace=True)\n",
    "        ret = pd.concat([ret, out_temp])\n",
    "\n",
    "    return ret\n",
    "\n",
    "def get_testing_flattened():\n",
    "    df = get_testing()\n",
    "    df = df.groupby(level = [0,2], axis = 1).mean()[['estimated']]\n",
    "    df.columns = df.columns.droplevel().tolist()\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'datetime': 'ds'}, inplace=True)\n",
    "    df['weather_data_type'] = 'estimated'\n",
    "    return df[['location', 'ds', 'weather_data_type'] + [i for i in df.columns.tolist() if i not in ['location', 'ds', 'y', 'weather_data_type']]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils-functions for making the submittable file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### UTILS FUNCTIONS ######\n",
    "def y_pred_to_csv(file_name, df):\n",
    "    df[['y_pred']].reset_index(drop=True).reset_index().rename(columns={'index': 'id', 'y_pred': 'prediction'}).to_csv(\"./submissions/\" + file_name, index=False)\n",
    "\n",
    "def make_submittable(file_name, model = None, model_dict = None):\n",
    "    \"\"\"\n",
    "    model if same model used for all locations\n",
    "    model dict if not\n",
    "\n",
    "    model is instance of model (model = MetaModel())\n",
    "    model dict uses location as key ({\"A\": MetaModel(), \"B\": MetaModel(), \"C\": MetaModel()})\n",
    "    \"\"\"\n",
    "\n",
    "    df = get_training_cleaned()\n",
    "    test = get_testing_flattened()\n",
    "    ret = pd.DataFrame()\n",
    "    \n",
    "    for location in ['A', 'B', 'C']:\n",
    "        temp_df = df[df['location']==location]\n",
    "        temp_test = test[test['location']==location]\n",
    "\n",
    "        if model is not None:\n",
    "            m = model\n",
    "        elif model_dict is not None:\n",
    "            m = model_dict[location]\n",
    "        else:\n",
    "            raise ValueError(\"no model specified\")\n",
    "    \n",
    "        m.train(temp_df)\n",
    "        fcst = m.predict(temp_test)\n",
    "\n",
    "        ret = pd.concat([ret, fcst])\n",
    "\n",
    "    y_pred_to_csv(file_name, ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract Model-class\n",
    "Definition of an abstract model class that all models inherit. This helped streamline production of models and make comparable testing-figures through the test-functions that all inheriting models use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaModel(ABC):     \n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "\n",
    "        return\n",
    "\n",
    "    def test(self, df: pd.DataFrame, n_splits=5):\n",
    "        \"\"\"\n",
    "            K-fold cross-validation, df must have y in it for testing against predictions\n",
    "        \"\"\"\n",
    "        print(f\"Testing {self.model_name}\")\n",
    "        column_names = df.columns.tolist()\n",
    "        if 'y' not in column_names:\n",
    "            raise Exception(f\"Missing observed y in columns. Available are {column_names}\")\n",
    "\n",
    "        # This is unecessary because we already clean it when calling train\n",
    "        # drop_y_with_na\n",
    "        df = df.dropna(subset=['y'], inplace=False)\n",
    "\n",
    "        MAE_values = []\n",
    "        MSE_values = []\n",
    "\n",
    "        # tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        kf =KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "        for train_index, test_index in kf.split(df):\n",
    "            train_partition = df.iloc[train_index]\n",
    "            valid_partition = df.iloc[test_index]\n",
    "\n",
    "            self.train(train_partition)\n",
    "            predictions = self.predict(valid_partition)\n",
    "            \n",
    "            y_true = valid_partition['y']\n",
    "            y_pred = predictions['y_pred']\n",
    "\n",
    "            MAE = mean_absolute_error(y_true, y_pred)\n",
    "            MAE_values.append(MAE)\n",
    "\n",
    "            MSE_values.append((y_pred - y_true).mean())\n",
    "\n",
    "            print(f\"Run {len(MAE_values)} MAE =\", MAE)\n",
    "\n",
    "        print(\"Mean Signed Error vals\", MSE_values)\n",
    "        average_mae = statistics.mean(MAE_values)\n",
    "        print(\"MAE Vals: MEAN:\", average_mae, 'ALL:' , MAE_values)\n",
    "        \n",
    "        return MAE_values\n",
    "    \n",
    "    @abstractmethod\n",
    "    def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "            Takes in single-index (datetime as index) df, and returns df with only desired features\n",
    "        \"\"\"    \n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def train(df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "            Runs trained model on on input df, preprocessing the df first and then returns datetime and y_pred\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters were selected by doing a grid-search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatBoostHenrik(MetaModel):\n",
    "    \n",
    "    def __init__(self, features=None, random_state=42):\n",
    "        super().__init__(\"CatBoost\")\n",
    "\n",
    "        self.random_state = random_state\n",
    "\n",
    "        if(features):\n",
    "            self.features = features\n",
    "        else:\n",
    "            self.features = []\n",
    "            self.features.extend(['sample_importance',\n",
    "                                'dayofyear',\n",
    "                                'hour',\n",
    "                                'total_rad_1h:J',\n",
    "                                'is_day:idx',\n",
    "            'absolute_humidity_2m:gm3',\n",
    "            'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J',\n",
    "            'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx',\n",
    "            'dew_point_2m:K', 'effective_cloud_cover:p', 'elevation:m',\n",
    "            'fresh_snow_12h:cm', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm',\n",
    "            'fresh_snow_3h:cm', 'fresh_snow_6h:cm',\n",
    "            'is_in_shadow:idx', 'msl_pressure:hPa', 'precip_5min:mm',\n",
    "            'precip_type_5min:idx', 'pressure_100m:hPa', 'pressure_50m:hPa',\n",
    "            'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "            'sfc_pressure:hPa', 'snow_density:kgm3', 'snow_depth:cm',\n",
    "            'snow_drift:idx', 'snow_melt_10min:mm', 'snow_water:kgm2',\n",
    "            'sun_azimuth:d', 'sun_elevation:d', 'super_cooled_liquid_water:kgm2',\n",
    "            't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "            'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms',\n",
    "            'wind_speed_w_1000hPa:ms'])\n",
    "\n",
    "    def preprocess(self, df: pd.DataFrame):\n",
    "        temp_df = df.copy()\n",
    "\n",
    "        has_target = 'y' in temp_df.columns        \n",
    "        \n",
    "        ##################################################################################### \n",
    "        # FEATURE ENGINEERING\n",
    "        #####################################################################################\n",
    "\n",
    "         # Emphasize test start-end: Starting date: 2023-05-01 00:00:00 Ending data 2023-07-03 23:00:00\n",
    "        temp_df['sample_importance'] = 1\n",
    "        temp_df.loc[(temp_df['ds'].dt.month >= 5) & \n",
    "                    (temp_df['ds'].dt.month < 7), 'sample_importance'] = 2\n",
    "        \n",
    "        temp_df.loc[(temp_df['ds'].dt.month == 7) &\n",
    "                    (temp_df['ds'].dt.day <= 4), 'sample_importance'] = 2\n",
    "        \n",
    "        # Add is_estimated parameter\n",
    "        temp_df['is_estimated'] = (temp_df['weather_data_type'] == 'estimated')\n",
    "        temp_df['is_estimated'] = temp_df['is_estimated'].astype(int)\n",
    "\n",
    "        temp_df['total_rad_1h:J'] = temp_df['diffuse_rad_1h:J'] + temp_df['direct_rad_1h:J']    \n",
    "        \n",
    "        # Extracting hour-of-day and month, and making them cyclical\n",
    "        temp_df['hour'] = temp_df['ds'].dt.hour\n",
    "        temp_df['hour'] = (np.sin(2 * np.pi * (temp_df['hour'] - 4)/ 24) + 1) / 2\n",
    "\n",
    "        temp_df['month'] = temp_df['ds'].dt.month\n",
    "        temp_df['month'] = (np.sin(2 * np.pi * (temp_df['month'])/ 12) + 1) / 2\n",
    "\n",
    "        temp_df['dayofyear'] = temp_df['ds'].dt.day_of_year\n",
    "        temp_df['dayofyear'] = np.sin(2 * np.pi * (temp_df['dayofyear'] - 80)/ 365)\n",
    "   \n",
    "        # SETTING NAN TO 0 CONFORMING TO XGBOOST\n",
    "        temp_df.fillna(0, inplace=True)\n",
    "        #####################################################################################\n",
    "\n",
    "        # DROPPING UNEEEDED FEATURES\n",
    "        if(has_target):\n",
    "            features_w_y = self.features + ['y']\n",
    "            temp_df = temp_df[features_w_y]\n",
    "\n",
    "        else:\n",
    "            temp_df = temp_df[self.features]\n",
    "\n",
    "        return temp_df\n",
    "\n",
    "    def train(self, df):\n",
    "        temp_df = self.preprocess(df)\n",
    "\n",
    "        # Separate features and target\n",
    "        X = temp_df.drop('y', axis=1, inplace=False).copy().values\n",
    "        y = temp_df['y'].copy().values\n",
    "\n",
    "        # Train test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9, random_state=self.random_state)\n",
    "\n",
    "        # Parameters found through grid-search and some manual tuning (CatBoostRegressor.grid_search())\n",
    "        params = {\n",
    "            'objective': \"MAE\",\n",
    "            'learning_rate': 0.02,\n",
    "            'depth': 6,\n",
    "            'iterations': 8000,\n",
    "            'logging_level': 'Silent',\n",
    "            'l2_leaf_reg': 5\n",
    "        }\n",
    "\n",
    "        self.model = cb.CatBoostRegressor(**params)\n",
    "        \n",
    "        self.model.fit(\n",
    "             X_train,\n",
    "             y_train,\n",
    "             verbose=True,\n",
    "             eval_set=(X_test, y_test),\n",
    "        )\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        df = self.preprocess(df)\n",
    "\n",
    "        features = [col for col in df.columns if col != 'y']\n",
    "        X = df[features].values\n",
    "        y_preds = self.model.predict(X)\n",
    "\n",
    "        # Set all negative predictions to 0\n",
    "        y_preds = np.maximum(y_preds, 0)\n",
    "\n",
    "        out_df = pd.DataFrame(data={'y_pred': y_preds})\n",
    "\n",
    "        return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost Composite\n",
    "This model takes an arithmetic average of 20 CatBoost models. \n",
    "\n",
    "To make the individual CatBoost models slightly different and less prone to overfitting, we assign the models slightly different features. The features that were found to have particular importance in the EDA, were used as the common features. All other features, except for those removed as described in the cleaning and preprocessing were randomly assigned. Through experimentation we found that this composite model performed best when each individual model randomly selected 90% of the models in the random-pool.\n",
    "\n",
    "In addition to this, the models have different train-test splits by being provided different random-seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatCompositeHenrik(MetaModel):\n",
    "    \n",
    "    def __init__(self, num_models=20):\n",
    "        super().__init__(\"CatComposite Henrik\")\n",
    "\n",
    "        self.num_models = num_models\n",
    "        self.common_features = ['sample_importance', 'is_estimated','dayofyear',\n",
    "                                'is_day:idx',\n",
    "                             'hour', 'month',\n",
    "                            'total_rad_1h:J',\n",
    "                            'sun_elevation:d',\n",
    "                            'sun_azimuth:d',\n",
    "                            'is_in_shadow:idx',\n",
    "                            'effective_cloud_cover:p']\n",
    "        \n",
    "        self.random_features = ['absolute_humidity_2m:gm3',\n",
    "                                'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J',\n",
    "                                'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx',\n",
    "                                'dew_point_2m:K', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W',\n",
    "                                'direct_rad_1h:J', 'fresh_snow_3h:cm',\n",
    "                                'precip_5min:mm','precip_type_5min:idx', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "                                'sfc_pressure:hPa','snow_water:kgm2',\n",
    "                                'super_cooled_liquid_water:kgm2',\n",
    "                                't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "                                'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms']\n",
    "             \n",
    "    def preprocess(self, df: pd.DataFrame):\n",
    "        return df.copy()\n",
    "    \n",
    "    def train(self, df: pd.DataFrame):\n",
    "        num_models = self.num_models\n",
    "\n",
    "        # How many of the features to be random, this gives some variations in all data and reduces overfitting.\n",
    "        # Each model picks 90% of the features from the \"random_features\" pool. 90% was found through experimenting\n",
    "        # and have given the best results.\n",
    "        num_rand_features = round(len(self.random_features) * 0.9)\n",
    "\n",
    "        df = df.copy()\n",
    "        df['month'] = df['ds'].dt.month\n",
    "\n",
    "        # Manually set random_states to be used for each models for reproducability\n",
    "        random_states = [i for i in range(33, 33 + num_models)]\n",
    "\n",
    "        # DUPLICATE MONTHS WE PREDICT\n",
    "        selected_months = df[(df['month'] == 5) | (df['month'] == 6) | (df['month'] == 7)].copy()\n",
    "        train_df = pd.concat([df, selected_months], ignore_index=True)\n",
    "\n",
    "        features = dict()\n",
    "        self.models = dict()\n",
    "\n",
    "        # Set-up models\n",
    "        for i in range(num_models):\n",
    "            random.seed(random_states[i])\n",
    "            temp_rand_features = random.sample(self.random_features, num_rand_features)\n",
    "            features[i] = self.common_features + temp_rand_features\n",
    "            self.models[f'CATBOOST_{i}'] = CatBoostHenrik(features = features[i], random_state=random_states[i])\n",
    "\n",
    "        # Train each model\n",
    "        for key in self.models:\n",
    "            print(\"Training model\", key)\n",
    "            self.models[key].train(train_df)     \n",
    "    \n",
    "    def predict(self, df):\n",
    "        all_preds = None\n",
    "        out_df = None\n",
    "\n",
    "        # Get predictions from all the sub-models\n",
    "        for key in self.models:\n",
    "            y_pred = self.models[key].predict(df)['y_pred']\n",
    "            if(all_preds is None):\n",
    "                all_preds = pd.DataFrame(y_pred)\n",
    "            else:\n",
    "                all_preds[key] = y_pred.values\n",
    "\n",
    "        # Take equally weighted average of models\n",
    "        out_np = all_preds.mean(axis=1)\n",
    "\n",
    "        # Make negative values 0\n",
    "        out_np = np.maximum(out_np, 0)\n",
    "\n",
    "        return pd.DataFrame(out_np, columns=['y_pred'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross-validation for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf = ml.data.get_training_cleaned()\\n\\nfor location in [\\'A\\', \\'B\\', \\'C\\']:\\n    print(f\"###############  LOCATION {location} ###############\")\\n    df_location = df[df[\\'location\\'] == location]\\n    \\n    cch = CatCompositeHenrik()\\n    cch.test(df_location)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df = ml.data.get_training_cleaned()\n",
    "\n",
    "for location in ['A', 'B', 'C']:\n",
    "    print(f\"###############  LOCATION {location} ###############\")\n",
    "    df_location = df[df['location'] == location]\n",
    "    \n",
    "    cch = CatCompositeHenrik()\n",
    "    cch.test(df_location)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autogluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard autogluon model with hyperparameter optimization enabled, tuning data set to a random sample from months 5 and 6 (which are the months we are predicting for), and sample weighting set to 2 for months 5 and 6. In preprocessing, we also added dayofweek and hour with a sin transformation and month without any transformation. We also added a 'total_rad_1h:J' feature\n",
    "\n",
    "    temp_df['total_rad_1h:J'] = temp_df['diffuse_rad_1h:J'] + temp_df['direct_rad_1h:J']    \n",
    "\n",
    "\n",
    "This combination of settings was decided based on which kaggle submission was best.\n",
    "\n",
    "This needs to be run on a similar setup to produce the same results because of the time limit. I used a 2020 M1 MacBook Air with 8GB of RAM. It should regardless produce a similar kaggle submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AutoGluonJacob(MetaModel):\n",
    "    \n",
    "    def __init__(self, time_limit=60*20):\n",
    "        super().__init__(\"AutoGluon Jacob\")\n",
    "\n",
    "        # autogluon features\n",
    "        # TabularPredictor (usage : **params_TabularPredictor)\n",
    "        self.params_TabularPredictor = \\\n",
    "            {\n",
    "                'label': 'y',\n",
    "                'problem_type': 'regression', \n",
    "                'eval_metric': 'mean_absolute_error',\n",
    "                'verbosity': 2,\n",
    "            }\n",
    "        # TabularPredictor.fit\n",
    "        self.params_TabularPredictor_fit = \\\n",
    "            {\n",
    "                'time_limit': time_limit,\n",
    "                'presets': 'high_quality', # [‘best_quality’, ‘high_quality’, ‘good_quality’, ‘medium_quality’, ‘optimize_for_deployment’, ‘interpretable’, ‘ignore_text’]\n",
    "                'hyperparameters': 'default',\n",
    "                # 'auto_stack': False,\n",
    "                # 'num_bag_folds': None, # set automatically by auto_stack True\n",
    "                # 'num_bag_sets': None, # set to 20 because of auto_stack\n",
    "                # 'num_stack_levels': 3, # set automatically by auto_stack True\n",
    "                'hyperparameter_tune_kwargs': 'random', # None to disable\n",
    "                # 'refit_full': True,\n",
    "                # 'feature_prune_kwargs': {}, # If None, do not perform feature pruning. If empty dictionary, perform feature pruning with default configurations.\n",
    "            }\n",
    "\n",
    "        self.use_tuning_data = True # 'sample_weight', 'random'\n",
    "        self.use_sample_weight = True\n",
    "\n",
    "        if self.use_sample_weight: # auto_weight a feature that exists\n",
    "            self.params_TabularPredictor['sample_weight'] = 'sample_importance'\n",
    "\n",
    "        # self.common_features = ['sample_importance', 'weather_data_type','dayofyear',\n",
    "        #                         'is_day:idx',\n",
    "        #                      'hour', 'month',\n",
    "        #                     'total_rad_1h:J',\n",
    "        #                     'sun_elevation:d',\n",
    "        #                     'sun_azimuth:d',\n",
    "        #                     'is_in_shadow:idx',\n",
    "        #                     'effective_cloud_cover:p']\n",
    "        \n",
    "        # self.random_features = ['absolute_humidity_2m:gm3',\n",
    "        #                         'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J',\n",
    "        #                         'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx',\n",
    "        #                         'dew_point_2m:K', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W',\n",
    "        #                         'direct_rad_1h:J', 'fresh_snow_3h:cm',\n",
    "        #                         'precip_5min:mm','precip_type_5min:idx', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "        #                         'sfc_pressure:hPa','snow_water:kgm2',\n",
    "        #                         'super_cooled_liquid_water:kgm2',\n",
    "        #                         't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "        #                         'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms']\n",
    "        \n",
    "\n",
    "    def preprocess(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        temp_df = df.copy()\n",
    "\n",
    "        temp_df['total_rad_1h:J'] = temp_df['diffuse_rad_1h:J'] + temp_df['direct_rad_1h:J']    \n",
    "        \n",
    "        # Extracting hour-of-day and month, and making them cyclical\n",
    "        temp_df['hour'] = temp_df['ds'].dt.hour\n",
    "        temp_df['hour'] = (np.sin(2 * np.pi * (temp_df['hour'] - 4)/ 24) + 1) / 2\n",
    "\n",
    "        temp_df['dayofyear'] = temp_df['ds'].dt.day_of_year\n",
    "        temp_df['dayofyear'] = np.sin(2 * np.pi * (temp_df['dayofyear'] - 80)/ 365)\n",
    "\n",
    "        # temp_df['year'] = temp_df['ds'].dt.hour\n",
    "        temp_df['month'] = temp_df['ds'].dt.month\n",
    "        # temp_df['day'] = temp_df['ds'].dt.day\n",
    "        # temp_df['dayofweek'] = temp_df['ds'].dt.dayofweek\n",
    "\n",
    "        if self.use_sample_weight:\n",
    "            # Emphasize test start-end: Starting date: 2023-05-01 00:00:00 Ending data 2023-07-03 23:00:00\n",
    "            temp_df['sample_importance'] = 1\n",
    "            temp_df.loc[(temp_df['ds'].dt.month >= 5) & \n",
    "                        (temp_df['ds'].dt.month < 7), 'sample_importance'] = 2\n",
    "            \n",
    "            # temp_df.loc[(temp_df['ds'].dt.month == 7) &\n",
    "            #             (temp_df['ds'].dt.day <= 4), 'sample_importance'] = 2\n",
    "\n",
    "\n",
    "\n",
    "        return temp_df.drop(columns=['ds'])#[self.common_features + self.random_features + (['y'] if 'y' in temp_df else [])].copy()\n",
    "\n",
    "    def train(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        temp_df = self.preprocess(df)\n",
    "\n",
    "        if self.use_tuning_data:\n",
    "\n",
    "            tuning_data = temp_df[(temp_df['month'] == 5) | (temp_df['month'] == 6)].sample(frac=0.5, random_state=42)\n",
    "            train_data = TabularDataset(temp_df[~temp_df.isin(tuning_data.to_dict(orient='list')).all(1)])\n",
    "            # train_data, tuning_data = train_test_split(df, test_size=0.1, random_state=42)\n",
    "            train_data = TabularDataset(train_data)\n",
    "\n",
    "            self.model = TabularPredictor(**self.params_TabularPredictor).fit(train_data, tuning_data=tuning_data, use_bag_holdout=True, **self.params_TabularPredictor_fit)\n",
    "        else:\n",
    "            train_data = TabularDataset(temp_df)\n",
    "\n",
    "            self.model = TabularPredictor(**self.params_TabularPredictor).fit(train_data, **self.params_TabularPredictor_fit)\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        print(\"Predict called\")\n",
    "        df = self.preprocess(df)\n",
    "\n",
    "        features = [col for col in df.columns if col != 'y']\n",
    "        X = df[features]\n",
    "\n",
    "\n",
    "\n",
    "        y_preds = self.model.predict(X)\n",
    "        # print(self.model.leaderboard())\n",
    "       \n",
    "\n",
    "        out_df = pd.DataFrame(data={'y_pred': y_preds})\n",
    "\n",
    "        return out_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class IntermediateAverageModel(MetaModel):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"IntermediateAverageModel\")\n",
    "\n",
    "        # autogluon features\n",
    "        # TabularPredictor (usage : **params_TabularPredictor)\n",
    "        \n",
    "\n",
    "    def preprocess(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        temp_df = df.copy()\n",
    "\n",
    "    def train(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        temp_df = self.preprocess(df)\n",
    "\n",
    "        self.models = {\n",
    "            #\"XGBoost Composite\": XGBoostComposite(),\n",
    "            #\"AutoGluon 5min\": AutoGluonHenrik(time_limit=60*30, excluded_models=excluded_ag_models),\n",
    "            \"AutoGluonJacob\": AutoGluonJacob(time_limit=60*20),\n",
    "            \"CatComposite x20\": CatCompositeHenrik(num_models=20)\n",
    "        }\n",
    "\n",
    "        for key in self.models:\n",
    "            self.models[key].train(df)\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        df = self.preprocess(df)\n",
    "\n",
    "        print(\"Predict called\")\n",
    "        all_preds = None\n",
    "\n",
    "        out_df = None\n",
    "\n",
    "        for key in self.models:\n",
    "            y_pred = self.models[key].predict(df)['y_pred']\n",
    "            if(all_preds is None):\n",
    "                all_preds = pd.DataFrame(y_pred)\n",
    "            else:\n",
    "                all_preds[key] = y_pred.values\n",
    "\n",
    "            \n",
    "        avg_series = all_preds.mean(axis=1)\n",
    "\n",
    "        # print(\"The different models produced the following predictions:\")\n",
    "        # print(all_preds)\n",
    "\n",
    "        avg_series = np.maximum(avg_series, 0)\n",
    "\n",
    "        return pd.DataFrame(avg_series, columns=['y_pred'])\n",
    "    ##################\n",
    "\n",
    "\n",
    "        # features = [col for col in df.columns if col != 'y']\n",
    "        # X = df[features]\n",
    "        # y_preds = self.model.predict(X)\n",
    "        # # print(self.model.leaderboard())\n",
    "        # out_df = pd.DataFrame(data={'y_pred': y_preds})\n",
    "\n",
    "        # return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Submittable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model CATBOOST_0\n",
      "Training model CATBOOST_0\n",
      "Training model CATBOOST_0\n"
     ]
    }
   ],
   "source": [
    "# Generate submittable\n",
    "make_submittable(\"CatComposite_short_notebook.csv\",\n",
    "                           model=IntermediateAverageModel())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
